{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0919f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('names.txt', 'r') as f:\n",
    "    names = f.readlines()\n",
    "\n",
    "chars =  ['.'] + sorted(set( ch for name in names for ch in name.strip())) \n",
    "\n",
    "print(type(chars))\n",
    "print(chars)\n",
    "stoi = { ch:i for i, ch in enumerate(chars)}\n",
    "itos = { i:ch for i, ch in enumerate(chars)}\n",
    "\n",
    "print(stoi)\n",
    "print(itos)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b72f87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 10\n",
    "context_length = 3\n",
    "hidden_size = 100\n",
    "mini_batch = 32\n",
    "epochs = 20000\n",
    "lr = 0.1\n",
    "weight_decay = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db44cec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train  = []\n",
    "Y_train = []\n",
    "X_val = []\n",
    "Y_val = []\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(names)\n",
    "total_len = len(names)\n",
    "train_len = int(total_len * 0.9)\n",
    "val_len = total_len - train_len\n",
    "train_names = names[:train_len]\n",
    "val_names = names[train_len:]\n",
    "for name in train_names:\n",
    "    name =  name.strip()\n",
    "    context = [0] * context_length\n",
    "    for ch in name + '.':\n",
    "        input = [itos[c] for c in context]\n",
    "        \n",
    "        X_train.append(context)\n",
    "        Y_train.append(stoi[ch])\n",
    "        context = context[1:] + [stoi[ch]]\n",
    "for name in val_names:\n",
    "    name =  name.strip()\n",
    "    context = [0] * context_length\n",
    "    for ch in name + '.':\n",
    "        input = [itos[c] for c in context]\n",
    "        \n",
    "        X_val.append(context)\n",
    "        Y_val.append(stoi[ch])\n",
    "        context = context[1:] + [stoi[ch]]\n",
    "\n",
    "import torch\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "X_train = torch.tensor(X_train)\n",
    "Y_train = torch.tensor(Y_train)\n",
    "X_val = torch.tensor(X_val)\n",
    "Y_val = torch.tensor(Y_val)\n",
    "\n",
    "C = torch.randn((27, embedding_size), generator=g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563834a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "g= torch.Generator().manual_seed(2147483647)\n",
    "class Linear:\n",
    "    def __init__(self, n_in, n_out, bias=True):\n",
    "        self.w = torch.randn((n_in, n_out), generator=g) * (5/3) * (1/ (n_in)**0.5)\n",
    "        self.b = torch.randn((n_out), generator=g) if bias else None\n",
    "        self.bias = bias \n",
    "    \n",
    "    def __call__(self, x):\n",
    "        self.out = x @ self.w\n",
    "        if self.bias:\n",
    "            self.out += self.b\n",
    "        return self.out\n",
    "    \n",
    "    def parameters(self):\n",
    "        if self.bias:\n",
    "            return [self.w, self.b]\n",
    "        else:\n",
    "            return [self.w]\n",
    "    \n",
    "class Tanh:\n",
    "    def __call__(self, x):\n",
    "        self.out = torch.tanh(x)\n",
    "        return self.out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return []\n",
    "    \n",
    "\n",
    "class BatchNorm1d:\n",
    "    def __init__(self, dim, eps=1e-05, momentum=0.1):\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "        self.training = True\n",
    "        self.scale = torch.ones(dim)\n",
    "        self.shift = torch.zeros(dim)\n",
    "        self.running_mean = torch.zeros(dim)\n",
    "        self.running_var = torch.ones(dim)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        if self.training:\n",
    "            xmean = x.mean(dim=0, keepdim=True)\n",
    "            xvar = x.var(dim=0, keepdim=True)\n",
    "        else:\n",
    "            xmean = self.running_mean\n",
    "            xvar = self.running_var\n",
    "        I_std = xvar.sqrt() + self.eps\n",
    "        I_mean = xmean\n",
    "        x = (x - I_mean) / I_std\n",
    "        self.out = self.scale * x + self.shift\n",
    "        if self.training:\n",
    "            with torch.no_grad():\n",
    "                self.running_mean = self.momentum * self.running_mean + (1 - self.momentum) * I_mean\n",
    "                self.running_var = self.momentum * self.running_var + (1 - self.momentum) * I_std\n",
    "        return self.out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [self.scale, self.shift]\n",
    "    \n",
    "class CrossEntropyLoss:\n",
    "    def __call__(self, x, y):\n",
    "        return torch.nn.functional.cross_entropy(x, y)\n",
    "    \n",
    "\n",
    "            \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0a1c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "layers = [\n",
    "    Linear(context_length * embedding_size, hidden_size),  BatchNorm1d(hidden_size), Tanh(),\n",
    "    Linear(hidden_size, hidden_size, bias = False), BatchNorm1d(hidden_size), Tanh(),\n",
    "    Linear(hidden_size, hidden_size, bias = False), BatchNorm1d(hidden_size), Tanh(),\n",
    "    Linear(hidden_size, hidden_size, bias = False), BatchNorm1d(hidden_size), Tanh(),\n",
    "    Linear(hidden_size, hidden_size, bias = False), BatchNorm1d(hidden_size), Tanh(),\n",
    "    Linear(hidden_size, 27, bias = False), BatchNorm1d(27),\n",
    "]\n",
    "\n",
    "\n",
    "# layers = [\n",
    "#     Linear(context_length * embedding_size, hidden_size),  Tanh(),\n",
    "#     Linear(hidden_size, hidden_size, bias = False),  Tanh(),\n",
    "#     Linear(hidden_size, hidden_size, bias = False),  Tanh(),\n",
    "#     Linear(hidden_size, hidden_size, bias = False),  Tanh(),\n",
    "#     Linear(hidden_size, hidden_size, bias = False),  Tanh(),\n",
    "#     Linear(hidden_size, 27, bias = False), BatchNorm1d(27),\n",
    "# ]\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "  # last layer: make less confident\n",
    "  layers[-1].scale *= 0.1\n",
    "  #layers[-1].weight *= 0.1\n",
    "  # all other layers: apply gain\n",
    "  for layer in layers[:-1]:\n",
    "    if isinstance(layer, Linear):\n",
    "      layer.w *= 1.0 #5/3\n",
    "\n",
    "parameters = [C] + [p for layer in layers for p in layer.parameters()]\n",
    "\n",
    "print(sum(p.numel() for p in parameters))\n",
    "for p in parameters:\n",
    "  p.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716fb52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "ud = []\n",
    "um = []\n",
    "for i in range(epochs):\n",
    "    samples = torch.randint(0, X_train.shape[0], (mini_batch,))\n",
    "    mini_batch_X = X_train[samples]\n",
    "    mini_batch_Y = Y_train[samples]\n",
    "    E = C[mini_batch_X]\n",
    "    E = E.view(-1, context_length * embedding_size)\n",
    "    for layer in layers:\n",
    "        E = layer(E)\n",
    "    loss = CrossEntropyLoss()(E, mini_batch_Y)\n",
    "    for layer in layers:\n",
    "        layer.out.retain_grad()\n",
    "    for p in parameters:    \n",
    "        p.grad = None\n",
    "    \n",
    "    loss.backward()\n",
    "    losses.append(loss.item())\n",
    "    print(\"training loss: \", loss.item())\n",
    "    \n",
    "    for p in parameters:\n",
    "        p.data -= p.grad * lr\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        ud.append([((lr*p.grad).std() / p.data.std()).log10().item() for p in parameters])\n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5287df47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize histograms\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(20, 4)) # width and height of the plot\n",
    "legends = []\n",
    "for i, layer in enumerate(layers[:-1]): # note: exclude the output layer\n",
    "  if isinstance(layer, Tanh):\n",
    "    t = layer.out\n",
    "    print('layer %d (%10s): mean %+.2f, std %.2f, saturated: %.2f%%' % (i, layer.__class__.__name__, t.mean(), t.std(), (t.abs() > 0.97).float().mean()*100))\n",
    "    hy, hx = torch.histogram(t, density=True)\n",
    "    plt.plot(hx[:-1].detach(), hy.detach())\n",
    "    legends.append(f'layer {i} ({layer.__class__.__name__}')\n",
    "plt.legend(legends);\n",
    "plt.title('activation distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9dcc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize histograms\n",
    "plt.figure(figsize=(20, 4)) # width and height of the plot\n",
    "legends = []\n",
    "for i, layer in enumerate(layers[:-1]): # note: exclude the output layer\n",
    "    if isinstance(layer, Tanh):\n",
    "        t = layer.out.grad\n",
    "        print('layer %d (%10s): mean %+f, std %e' % (i, layer.__class__.__name__, t.mean(), t.std()))\n",
    "        hy, hx = torch.histogram(t, density=True)\n",
    "        plt.plot(hx[:-1].detach(), hy.detach())\n",
    "        legends.append(f'layer {i} ({layer.__class__.__name__}')\n",
    "plt.legend(legends);\n",
    "plt.title('gradient distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a5c54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize histograms\n",
    "plt.figure(figsize=(20, 4)) # width and height of the plot\n",
    "legends = []\n",
    "for l in layers:\n",
    "    for p in l.parameters():\n",
    "        t = p.grad\n",
    "        if p.ndim == 2:\n",
    "            print('weight %10s | mean %+f | std %e | grad:data ratio %e' % (tuple(p.shape), t.mean(), t.std(), t.std() / p.std()))\n",
    "            hy, hx = torch.histogram(t, density=True)\n",
    "            plt.plot(hx[:-1].detach(), hy.detach())\n",
    "            legends.append(f'{i} {tuple(p.shape)}')\n",
    "plt.legend(legends)\n",
    "plt.title('weights gradient distribution');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce80fd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 4))\n",
    "legends = []\n",
    "for i,p in enumerate(parameters):\n",
    "  if p.ndim == 2:\n",
    "    plt.plot([ud[j][i] for j in range(len(ud))])\n",
    "    legends.append('param %d' % i)\n",
    "plt.plot([0, len(ud)], [-3, -3], 'k') # these ratios should be ~1e-3, indicate on plot\n",
    "plt.legend(legends);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20669217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # BatchNorm forward pass as a widget\n",
    "\n",
    "# from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "# import ipywidgets as widgets\n",
    "# import scipy.stats as stats\n",
    "# import numpy as np\n",
    "\n",
    "# def normshow(x0):\n",
    "  \n",
    "#   g = torch.Generator().manual_seed(2147483647+1)\n",
    "#   x = torch.randn(5, generator=g) * 5\n",
    "#   x[0] = x0 # override the 0th example with the slider\n",
    "#   mu = x.mean()\n",
    "#   sig = x.std()\n",
    "#   y = (x - mu)/sig\n",
    "\n",
    "#   plt.figure(figsize=(10, 5))\n",
    "#   # plot 0\n",
    "#   plt.plot([-6,6], [0,0], 'k')\n",
    "#   # plot the mean and std\n",
    "#   xx = np.linspace(-6, 6, 100)\n",
    "#   plt.plot(xx, stats.norm.pdf(xx, mu, sig), 'b')\n",
    "#   xx = np.linspace(-6, 6, 100)\n",
    "#   plt.plot(xx, stats.norm.pdf(xx, 0, 1), 'r')\n",
    "#   # plot little lines connecting input and output\n",
    "#   for i in range(len(x)):\n",
    "#     plt.plot([x[i],y[i]], [1, 0], 'k', alpha=0.2)\n",
    "#   # plot the input and output values\n",
    "#   plt.scatter(x.data, torch.ones_like(x).data, c='b', s=100)\n",
    "#   plt.scatter(y.data, torch.zeros_like(y).data, c='r', s=100)\n",
    "#   plt.xlim(-6, 6)\n",
    "#   # title\n",
    "#   plt.title('input mu %.2f std %.2f' % (mu, sig))\n",
    "\n",
    "# interact(normshow, x0=(-30,30,0.5));\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8517bf37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
