{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "f1283988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "chars = set()\n",
    "with open(\"names.txt\", \"r\") as f:\n",
    "    names = f.read().splitlines()\n",
    "for name in names:\n",
    "    chars.update(list(name))\n",
    "chars = sorted(chars)\n",
    "chars = ['.'] + list(chars) \n",
    "print(chars)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "8ecf312a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'.': 0, 'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26, ('.', '.'): 27, ('.', 'a'): 28, ('.', 'b'): 29, ('.', 'c'): 30, ('.', 'd'): 31, ('.', 'e'): 32, ('.', 'f'): 33, ('.', 'g'): 34, ('.', 'h'): 35, ('.', 'i'): 36, ('.', 'j'): 37, ('.', 'k'): 38, ('.', 'l'): 39, ('.', 'm'): 40, ('.', 'n'): 41, ('.', 'o'): 42, ('.', 'p'): 43, ('.', 'q'): 44, ('.', 'r'): 45, ('.', 's'): 46, ('.', 't'): 47, ('.', 'u'): 48, ('.', 'v'): 49, ('.', 'w'): 50, ('.', 'x'): 51, ('.', 'y'): 52, ('.', 'z'): 53, ('a', '.'): 54, ('a', 'a'): 55, ('a', 'b'): 56, ('a', 'c'): 57, ('a', 'd'): 58, ('a', 'e'): 59, ('a', 'f'): 60, ('a', 'g'): 61, ('a', 'h'): 62, ('a', 'i'): 63, ('a', 'j'): 64, ('a', 'k'): 65, ('a', 'l'): 66, ('a', 'm'): 67, ('a', 'n'): 68, ('a', 'o'): 69, ('a', 'p'): 70, ('a', 'q'): 71, ('a', 'r'): 72, ('a', 's'): 73, ('a', 't'): 74, ('a', 'u'): 75, ('a', 'v'): 76, ('a', 'w'): 77, ('a', 'x'): 78, ('a', 'y'): 79, ('a', 'z'): 80, ('b', '.'): 81, ('b', 'a'): 82, ('b', 'b'): 83, ('b', 'c'): 84, ('b', 'd'): 85, ('b', 'e'): 86, ('b', 'f'): 87, ('b', 'g'): 88, ('b', 'h'): 89, ('b', 'i'): 90, ('b', 'j'): 91, ('b', 'k'): 92, ('b', 'l'): 93, ('b', 'm'): 94, ('b', 'n'): 95, ('b', 'o'): 96, ('b', 'p'): 97, ('b', 'q'): 98, ('b', 'r'): 99, ('b', 's'): 100, ('b', 't'): 101, ('b', 'u'): 102, ('b', 'v'): 103, ('b', 'w'): 104, ('b', 'x'): 105, ('b', 'y'): 106, ('b', 'z'): 107, ('c', '.'): 108, ('c', 'a'): 109, ('c', 'b'): 110, ('c', 'c'): 111, ('c', 'd'): 112, ('c', 'e'): 113, ('c', 'f'): 114, ('c', 'g'): 115, ('c', 'h'): 116, ('c', 'i'): 117, ('c', 'j'): 118, ('c', 'k'): 119, ('c', 'l'): 120, ('c', 'm'): 121, ('c', 'n'): 122, ('c', 'o'): 123, ('c', 'p'): 124, ('c', 'q'): 125, ('c', 'r'): 126, ('c', 's'): 127, ('c', 't'): 128, ('c', 'u'): 129, ('c', 'v'): 130, ('c', 'w'): 131, ('c', 'x'): 132, ('c', 'y'): 133, ('c', 'z'): 134, ('d', '.'): 135, ('d', 'a'): 136, ('d', 'b'): 137, ('d', 'c'): 138, ('d', 'd'): 139, ('d', 'e'): 140, ('d', 'f'): 141, ('d', 'g'): 142, ('d', 'h'): 143, ('d', 'i'): 144, ('d', 'j'): 145, ('d', 'k'): 146, ('d', 'l'): 147, ('d', 'm'): 148, ('d', 'n'): 149, ('d', 'o'): 150, ('d', 'p'): 151, ('d', 'q'): 152, ('d', 'r'): 153, ('d', 's'): 154, ('d', 't'): 155, ('d', 'u'): 156, ('d', 'v'): 157, ('d', 'w'): 158, ('d', 'x'): 159, ('d', 'y'): 160, ('d', 'z'): 161, ('e', '.'): 162, ('e', 'a'): 163, ('e', 'b'): 164, ('e', 'c'): 165, ('e', 'd'): 166, ('e', 'e'): 167, ('e', 'f'): 168, ('e', 'g'): 169, ('e', 'h'): 170, ('e', 'i'): 171, ('e', 'j'): 172, ('e', 'k'): 173, ('e', 'l'): 174, ('e', 'm'): 175, ('e', 'n'): 176, ('e', 'o'): 177, ('e', 'p'): 178, ('e', 'q'): 179, ('e', 'r'): 180, ('e', 's'): 181, ('e', 't'): 182, ('e', 'u'): 183, ('e', 'v'): 184, ('e', 'w'): 185, ('e', 'x'): 186, ('e', 'y'): 187, ('e', 'z'): 188, ('f', '.'): 189, ('f', 'a'): 190, ('f', 'b'): 191, ('f', 'c'): 192, ('f', 'd'): 193, ('f', 'e'): 194, ('f', 'f'): 195, ('f', 'g'): 196, ('f', 'h'): 197, ('f', 'i'): 198, ('f', 'j'): 199, ('f', 'k'): 200, ('f', 'l'): 201, ('f', 'm'): 202, ('f', 'n'): 203, ('f', 'o'): 204, ('f', 'p'): 205, ('f', 'q'): 206, ('f', 'r'): 207, ('f', 's'): 208, ('f', 't'): 209, ('f', 'u'): 210, ('f', 'v'): 211, ('f', 'w'): 212, ('f', 'x'): 213, ('f', 'y'): 214, ('f', 'z'): 215, ('g', '.'): 216, ('g', 'a'): 217, ('g', 'b'): 218, ('g', 'c'): 219, ('g', 'd'): 220, ('g', 'e'): 221, ('g', 'f'): 222, ('g', 'g'): 223, ('g', 'h'): 224, ('g', 'i'): 225, ('g', 'j'): 226, ('g', 'k'): 227, ('g', 'l'): 228, ('g', 'm'): 229, ('g', 'n'): 230, ('g', 'o'): 231, ('g', 'p'): 232, ('g', 'q'): 233, ('g', 'r'): 234, ('g', 's'): 235, ('g', 't'): 236, ('g', 'u'): 237, ('g', 'v'): 238, ('g', 'w'): 239, ('g', 'x'): 240, ('g', 'y'): 241, ('g', 'z'): 242, ('h', '.'): 243, ('h', 'a'): 244, ('h', 'b'): 245, ('h', 'c'): 246, ('h', 'd'): 247, ('h', 'e'): 248, ('h', 'f'): 249, ('h', 'g'): 250, ('h', 'h'): 251, ('h', 'i'): 252, ('h', 'j'): 253, ('h', 'k'): 254, ('h', 'l'): 255, ('h', 'm'): 256, ('h', 'n'): 257, ('h', 'o'): 258, ('h', 'p'): 259, ('h', 'q'): 260, ('h', 'r'): 261, ('h', 's'): 262, ('h', 't'): 263, ('h', 'u'): 264, ('h', 'v'): 265, ('h', 'w'): 266, ('h', 'x'): 267, ('h', 'y'): 268, ('h', 'z'): 269, ('i', '.'): 270, ('i', 'a'): 271, ('i', 'b'): 272, ('i', 'c'): 273, ('i', 'd'): 274, ('i', 'e'): 275, ('i', 'f'): 276, ('i', 'g'): 277, ('i', 'h'): 278, ('i', 'i'): 279, ('i', 'j'): 280, ('i', 'k'): 281, ('i', 'l'): 282, ('i', 'm'): 283, ('i', 'n'): 284, ('i', 'o'): 285, ('i', 'p'): 286, ('i', 'q'): 287, ('i', 'r'): 288, ('i', 's'): 289, ('i', 't'): 290, ('i', 'u'): 291, ('i', 'v'): 292, ('i', 'w'): 293, ('i', 'x'): 294, ('i', 'y'): 295, ('i', 'z'): 296, ('j', '.'): 297, ('j', 'a'): 298, ('j', 'b'): 299, ('j', 'c'): 300, ('j', 'd'): 301, ('j', 'e'): 302, ('j', 'f'): 303, ('j', 'g'): 304, ('j', 'h'): 305, ('j', 'i'): 306, ('j', 'j'): 307, ('j', 'k'): 308, ('j', 'l'): 309, ('j', 'm'): 310, ('j', 'n'): 311, ('j', 'o'): 312, ('j', 'p'): 313, ('j', 'q'): 314, ('j', 'r'): 315, ('j', 's'): 316, ('j', 't'): 317, ('j', 'u'): 318, ('j', 'v'): 319, ('j', 'w'): 320, ('j', 'x'): 321, ('j', 'y'): 322, ('j', 'z'): 323, ('k', '.'): 324, ('k', 'a'): 325, ('k', 'b'): 326, ('k', 'c'): 327, ('k', 'd'): 328, ('k', 'e'): 329, ('k', 'f'): 330, ('k', 'g'): 331, ('k', 'h'): 332, ('k', 'i'): 333, ('k', 'j'): 334, ('k', 'k'): 335, ('k', 'l'): 336, ('k', 'm'): 337, ('k', 'n'): 338, ('k', 'o'): 339, ('k', 'p'): 340, ('k', 'q'): 341, ('k', 'r'): 342, ('k', 's'): 343, ('k', 't'): 344, ('k', 'u'): 345, ('k', 'v'): 346, ('k', 'w'): 347, ('k', 'x'): 348, ('k', 'y'): 349, ('k', 'z'): 350, ('l', '.'): 351, ('l', 'a'): 352, ('l', 'b'): 353, ('l', 'c'): 354, ('l', 'd'): 355, ('l', 'e'): 356, ('l', 'f'): 357, ('l', 'g'): 358, ('l', 'h'): 359, ('l', 'i'): 360, ('l', 'j'): 361, ('l', 'k'): 362, ('l', 'l'): 363, ('l', 'm'): 364, ('l', 'n'): 365, ('l', 'o'): 366, ('l', 'p'): 367, ('l', 'q'): 368, ('l', 'r'): 369, ('l', 's'): 370, ('l', 't'): 371, ('l', 'u'): 372, ('l', 'v'): 373, ('l', 'w'): 374, ('l', 'x'): 375, ('l', 'y'): 376, ('l', 'z'): 377, ('m', '.'): 378, ('m', 'a'): 379, ('m', 'b'): 380, ('m', 'c'): 381, ('m', 'd'): 382, ('m', 'e'): 383, ('m', 'f'): 384, ('m', 'g'): 385, ('m', 'h'): 386, ('m', 'i'): 387, ('m', 'j'): 388, ('m', 'k'): 389, ('m', 'l'): 390, ('m', 'm'): 391, ('m', 'n'): 392, ('m', 'o'): 393, ('m', 'p'): 394, ('m', 'q'): 395, ('m', 'r'): 396, ('m', 's'): 397, ('m', 't'): 398, ('m', 'u'): 399, ('m', 'v'): 400, ('m', 'w'): 401, ('m', 'x'): 402, ('m', 'y'): 403, ('m', 'z'): 404, ('n', '.'): 405, ('n', 'a'): 406, ('n', 'b'): 407, ('n', 'c'): 408, ('n', 'd'): 409, ('n', 'e'): 410, ('n', 'f'): 411, ('n', 'g'): 412, ('n', 'h'): 413, ('n', 'i'): 414, ('n', 'j'): 415, ('n', 'k'): 416, ('n', 'l'): 417, ('n', 'm'): 418, ('n', 'n'): 419, ('n', 'o'): 420, ('n', 'p'): 421, ('n', 'q'): 422, ('n', 'r'): 423, ('n', 's'): 424, ('n', 't'): 425, ('n', 'u'): 426, ('n', 'v'): 427, ('n', 'w'): 428, ('n', 'x'): 429, ('n', 'y'): 430, ('n', 'z'): 431, ('o', '.'): 432, ('o', 'a'): 433, ('o', 'b'): 434, ('o', 'c'): 435, ('o', 'd'): 436, ('o', 'e'): 437, ('o', 'f'): 438, ('o', 'g'): 439, ('o', 'h'): 440, ('o', 'i'): 441, ('o', 'j'): 442, ('o', 'k'): 443, ('o', 'l'): 444, ('o', 'm'): 445, ('o', 'n'): 446, ('o', 'o'): 447, ('o', 'p'): 448, ('o', 'q'): 449, ('o', 'r'): 450, ('o', 's'): 451, ('o', 't'): 452, ('o', 'u'): 453, ('o', 'v'): 454, ('o', 'w'): 455, ('o', 'x'): 456, ('o', 'y'): 457, ('o', 'z'): 458, ('p', '.'): 459, ('p', 'a'): 460, ('p', 'b'): 461, ('p', 'c'): 462, ('p', 'd'): 463, ('p', 'e'): 464, ('p', 'f'): 465, ('p', 'g'): 466, ('p', 'h'): 467, ('p', 'i'): 468, ('p', 'j'): 469, ('p', 'k'): 470, ('p', 'l'): 471, ('p', 'm'): 472, ('p', 'n'): 473, ('p', 'o'): 474, ('p', 'p'): 475, ('p', 'q'): 476, ('p', 'r'): 477, ('p', 's'): 478, ('p', 't'): 479, ('p', 'u'): 480, ('p', 'v'): 481, ('p', 'w'): 482, ('p', 'x'): 483, ('p', 'y'): 484, ('p', 'z'): 485, ('q', '.'): 486, ('q', 'a'): 487, ('q', 'b'): 488, ('q', 'c'): 489, ('q', 'd'): 490, ('q', 'e'): 491, ('q', 'f'): 492, ('q', 'g'): 493, ('q', 'h'): 494, ('q', 'i'): 495, ('q', 'j'): 496, ('q', 'k'): 497, ('q', 'l'): 498, ('q', 'm'): 499, ('q', 'n'): 500, ('q', 'o'): 501, ('q', 'p'): 502, ('q', 'q'): 503, ('q', 'r'): 504, ('q', 's'): 505, ('q', 't'): 506, ('q', 'u'): 507, ('q', 'v'): 508, ('q', 'w'): 509, ('q', 'x'): 510, ('q', 'y'): 511, ('q', 'z'): 512, ('r', '.'): 513, ('r', 'a'): 514, ('r', 'b'): 515, ('r', 'c'): 516, ('r', 'd'): 517, ('r', 'e'): 518, ('r', 'f'): 519, ('r', 'g'): 520, ('r', 'h'): 521, ('r', 'i'): 522, ('r', 'j'): 523, ('r', 'k'): 524, ('r', 'l'): 525, ('r', 'm'): 526, ('r', 'n'): 527, ('r', 'o'): 528, ('r', 'p'): 529, ('r', 'q'): 530, ('r', 'r'): 531, ('r', 's'): 532, ('r', 't'): 533, ('r', 'u'): 534, ('r', 'v'): 535, ('r', 'w'): 536, ('r', 'x'): 537, ('r', 'y'): 538, ('r', 'z'): 539, ('s', '.'): 540, ('s', 'a'): 541, ('s', 'b'): 542, ('s', 'c'): 543, ('s', 'd'): 544, ('s', 'e'): 545, ('s', 'f'): 546, ('s', 'g'): 547, ('s', 'h'): 548, ('s', 'i'): 549, ('s', 'j'): 550, ('s', 'k'): 551, ('s', 'l'): 552, ('s', 'm'): 553, ('s', 'n'): 554, ('s', 'o'): 555, ('s', 'p'): 556, ('s', 'q'): 557, ('s', 'r'): 558, ('s', 's'): 559, ('s', 't'): 560, ('s', 'u'): 561, ('s', 'v'): 562, ('s', 'w'): 563, ('s', 'x'): 564, ('s', 'y'): 565, ('s', 'z'): 566, ('t', '.'): 567, ('t', 'a'): 568, ('t', 'b'): 569, ('t', 'c'): 570, ('t', 'd'): 571, ('t', 'e'): 572, ('t', 'f'): 573, ('t', 'g'): 574, ('t', 'h'): 575, ('t', 'i'): 576, ('t', 'j'): 577, ('t', 'k'): 578, ('t', 'l'): 579, ('t', 'm'): 580, ('t', 'n'): 581, ('t', 'o'): 582, ('t', 'p'): 583, ('t', 'q'): 584, ('t', 'r'): 585, ('t', 's'): 586, ('t', 't'): 587, ('t', 'u'): 588, ('t', 'v'): 589, ('t', 'w'): 590, ('t', 'x'): 591, ('t', 'y'): 592, ('t', 'z'): 593, ('u', '.'): 594, ('u', 'a'): 595, ('u', 'b'): 596, ('u', 'c'): 597, ('u', 'd'): 598, ('u', 'e'): 599, ('u', 'f'): 600, ('u', 'g'): 601, ('u', 'h'): 602, ('u', 'i'): 603, ('u', 'j'): 604, ('u', 'k'): 605, ('u', 'l'): 606, ('u', 'm'): 607, ('u', 'n'): 608, ('u', 'o'): 609, ('u', 'p'): 610, ('u', 'q'): 611, ('u', 'r'): 612, ('u', 's'): 613, ('u', 't'): 614, ('u', 'u'): 615, ('u', 'v'): 616, ('u', 'w'): 617, ('u', 'x'): 618, ('u', 'y'): 619, ('u', 'z'): 620, ('v', '.'): 621, ('v', 'a'): 622, ('v', 'b'): 623, ('v', 'c'): 624, ('v', 'd'): 625, ('v', 'e'): 626, ('v', 'f'): 627, ('v', 'g'): 628, ('v', 'h'): 629, ('v', 'i'): 630, ('v', 'j'): 631, ('v', 'k'): 632, ('v', 'l'): 633, ('v', 'm'): 634, ('v', 'n'): 635, ('v', 'o'): 636, ('v', 'p'): 637, ('v', 'q'): 638, ('v', 'r'): 639, ('v', 's'): 640, ('v', 't'): 641, ('v', 'u'): 642, ('v', 'v'): 643, ('v', 'w'): 644, ('v', 'x'): 645, ('v', 'y'): 646, ('v', 'z'): 647, ('w', '.'): 648, ('w', 'a'): 649, ('w', 'b'): 650, ('w', 'c'): 651, ('w', 'd'): 652, ('w', 'e'): 653, ('w', 'f'): 654, ('w', 'g'): 655, ('w', 'h'): 656, ('w', 'i'): 657, ('w', 'j'): 658, ('w', 'k'): 659, ('w', 'l'): 660, ('w', 'm'): 661, ('w', 'n'): 662, ('w', 'o'): 663, ('w', 'p'): 664, ('w', 'q'): 665, ('w', 'r'): 666, ('w', 's'): 667, ('w', 't'): 668, ('w', 'u'): 669, ('w', 'v'): 670, ('w', 'w'): 671, ('w', 'x'): 672, ('w', 'y'): 673, ('w', 'z'): 674, ('x', '.'): 675, ('x', 'a'): 676, ('x', 'b'): 677, ('x', 'c'): 678, ('x', 'd'): 679, ('x', 'e'): 680, ('x', 'f'): 681, ('x', 'g'): 682, ('x', 'h'): 683, ('x', 'i'): 684, ('x', 'j'): 685, ('x', 'k'): 686, ('x', 'l'): 687, ('x', 'm'): 688, ('x', 'n'): 689, ('x', 'o'): 690, ('x', 'p'): 691, ('x', 'q'): 692, ('x', 'r'): 693, ('x', 's'): 694, ('x', 't'): 695, ('x', 'u'): 696, ('x', 'v'): 697, ('x', 'w'): 698, ('x', 'x'): 699, ('x', 'y'): 700, ('x', 'z'): 701, ('y', '.'): 702, ('y', 'a'): 703, ('y', 'b'): 704, ('y', 'c'): 705, ('y', 'd'): 706, ('y', 'e'): 707, ('y', 'f'): 708, ('y', 'g'): 709, ('y', 'h'): 710, ('y', 'i'): 711, ('y', 'j'): 712, ('y', 'k'): 713, ('y', 'l'): 714, ('y', 'm'): 715, ('y', 'n'): 716, ('y', 'o'): 717, ('y', 'p'): 718, ('y', 'q'): 719, ('y', 'r'): 720, ('y', 's'): 721, ('y', 't'): 722, ('y', 'u'): 723, ('y', 'v'): 724, ('y', 'w'): 725, ('y', 'x'): 726, ('y', 'y'): 727, ('y', 'z'): 728, ('z', '.'): 729, ('z', 'a'): 730, ('z', 'b'): 731, ('z', 'c'): 732, ('z', 'd'): 733, ('z', 'e'): 734, ('z', 'f'): 735, ('z', 'g'): 736, ('z', 'h'): 737, ('z', 'i'): 738, ('z', 'j'): 739, ('z', 'k'): 740, ('z', 'l'): 741, ('z', 'm'): 742, ('z', 'n'): 743, ('z', 'o'): 744, ('z', 'p'): 745, ('z', 'q'): 746, ('z', 'r'): 747, ('z', 's'): 748, ('z', 't'): 749, ('z', 'u'): 750, ('z', 'v'): 751, ('z', 'w'): 752, ('z', 'x'): 753, ('z', 'y'): 754, ('z', 'z'): 755}\n",
      "{0: '.', 1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 27: ('.', '.'), 28: ('.', 'a'), 29: ('.', 'b'), 30: ('.', 'c'), 31: ('.', 'd'), 32: ('.', 'e'), 33: ('.', 'f'), 34: ('.', 'g'), 35: ('.', 'h'), 36: ('.', 'i'), 37: ('.', 'j'), 38: ('.', 'k'), 39: ('.', 'l'), 40: ('.', 'm'), 41: ('.', 'n'), 42: ('.', 'o'), 43: ('.', 'p'), 44: ('.', 'q'), 45: ('.', 'r'), 46: ('.', 's'), 47: ('.', 't'), 48: ('.', 'u'), 49: ('.', 'v'), 50: ('.', 'w'), 51: ('.', 'x'), 52: ('.', 'y'), 53: ('.', 'z'), 54: ('a', '.'), 55: ('a', 'a'), 56: ('a', 'b'), 57: ('a', 'c'), 58: ('a', 'd'), 59: ('a', 'e'), 60: ('a', 'f'), 61: ('a', 'g'), 62: ('a', 'h'), 63: ('a', 'i'), 64: ('a', 'j'), 65: ('a', 'k'), 66: ('a', 'l'), 67: ('a', 'm'), 68: ('a', 'n'), 69: ('a', 'o'), 70: ('a', 'p'), 71: ('a', 'q'), 72: ('a', 'r'), 73: ('a', 's'), 74: ('a', 't'), 75: ('a', 'u'), 76: ('a', 'v'), 77: ('a', 'w'), 78: ('a', 'x'), 79: ('a', 'y'), 80: ('a', 'z'), 81: ('b', '.'), 82: ('b', 'a'), 83: ('b', 'b'), 84: ('b', 'c'), 85: ('b', 'd'), 86: ('b', 'e'), 87: ('b', 'f'), 88: ('b', 'g'), 89: ('b', 'h'), 90: ('b', 'i'), 91: ('b', 'j'), 92: ('b', 'k'), 93: ('b', 'l'), 94: ('b', 'm'), 95: ('b', 'n'), 96: ('b', 'o'), 97: ('b', 'p'), 98: ('b', 'q'), 99: ('b', 'r'), 100: ('b', 's'), 101: ('b', 't'), 102: ('b', 'u'), 103: ('b', 'v'), 104: ('b', 'w'), 105: ('b', 'x'), 106: ('b', 'y'), 107: ('b', 'z'), 108: ('c', '.'), 109: ('c', 'a'), 110: ('c', 'b'), 111: ('c', 'c'), 112: ('c', 'd'), 113: ('c', 'e'), 114: ('c', 'f'), 115: ('c', 'g'), 116: ('c', 'h'), 117: ('c', 'i'), 118: ('c', 'j'), 119: ('c', 'k'), 120: ('c', 'l'), 121: ('c', 'm'), 122: ('c', 'n'), 123: ('c', 'o'), 124: ('c', 'p'), 125: ('c', 'q'), 126: ('c', 'r'), 127: ('c', 's'), 128: ('c', 't'), 129: ('c', 'u'), 130: ('c', 'v'), 131: ('c', 'w'), 132: ('c', 'x'), 133: ('c', 'y'), 134: ('c', 'z'), 135: ('d', '.'), 136: ('d', 'a'), 137: ('d', 'b'), 138: ('d', 'c'), 139: ('d', 'd'), 140: ('d', 'e'), 141: ('d', 'f'), 142: ('d', 'g'), 143: ('d', 'h'), 144: ('d', 'i'), 145: ('d', 'j'), 146: ('d', 'k'), 147: ('d', 'l'), 148: ('d', 'm'), 149: ('d', 'n'), 150: ('d', 'o'), 151: ('d', 'p'), 152: ('d', 'q'), 153: ('d', 'r'), 154: ('d', 's'), 155: ('d', 't'), 156: ('d', 'u'), 157: ('d', 'v'), 158: ('d', 'w'), 159: ('d', 'x'), 160: ('d', 'y'), 161: ('d', 'z'), 162: ('e', '.'), 163: ('e', 'a'), 164: ('e', 'b'), 165: ('e', 'c'), 166: ('e', 'd'), 167: ('e', 'e'), 168: ('e', 'f'), 169: ('e', 'g'), 170: ('e', 'h'), 171: ('e', 'i'), 172: ('e', 'j'), 173: ('e', 'k'), 174: ('e', 'l'), 175: ('e', 'm'), 176: ('e', 'n'), 177: ('e', 'o'), 178: ('e', 'p'), 179: ('e', 'q'), 180: ('e', 'r'), 181: ('e', 's'), 182: ('e', 't'), 183: ('e', 'u'), 184: ('e', 'v'), 185: ('e', 'w'), 186: ('e', 'x'), 187: ('e', 'y'), 188: ('e', 'z'), 189: ('f', '.'), 190: ('f', 'a'), 191: ('f', 'b'), 192: ('f', 'c'), 193: ('f', 'd'), 194: ('f', 'e'), 195: ('f', 'f'), 196: ('f', 'g'), 197: ('f', 'h'), 198: ('f', 'i'), 199: ('f', 'j'), 200: ('f', 'k'), 201: ('f', 'l'), 202: ('f', 'm'), 203: ('f', 'n'), 204: ('f', 'o'), 205: ('f', 'p'), 206: ('f', 'q'), 207: ('f', 'r'), 208: ('f', 's'), 209: ('f', 't'), 210: ('f', 'u'), 211: ('f', 'v'), 212: ('f', 'w'), 213: ('f', 'x'), 214: ('f', 'y'), 215: ('f', 'z'), 216: ('g', '.'), 217: ('g', 'a'), 218: ('g', 'b'), 219: ('g', 'c'), 220: ('g', 'd'), 221: ('g', 'e'), 222: ('g', 'f'), 223: ('g', 'g'), 224: ('g', 'h'), 225: ('g', 'i'), 226: ('g', 'j'), 227: ('g', 'k'), 228: ('g', 'l'), 229: ('g', 'm'), 230: ('g', 'n'), 231: ('g', 'o'), 232: ('g', 'p'), 233: ('g', 'q'), 234: ('g', 'r'), 235: ('g', 's'), 236: ('g', 't'), 237: ('g', 'u'), 238: ('g', 'v'), 239: ('g', 'w'), 240: ('g', 'x'), 241: ('g', 'y'), 242: ('g', 'z'), 243: ('h', '.'), 244: ('h', 'a'), 245: ('h', 'b'), 246: ('h', 'c'), 247: ('h', 'd'), 248: ('h', 'e'), 249: ('h', 'f'), 250: ('h', 'g'), 251: ('h', 'h'), 252: ('h', 'i'), 253: ('h', 'j'), 254: ('h', 'k'), 255: ('h', 'l'), 256: ('h', 'm'), 257: ('h', 'n'), 258: ('h', 'o'), 259: ('h', 'p'), 260: ('h', 'q'), 261: ('h', 'r'), 262: ('h', 's'), 263: ('h', 't'), 264: ('h', 'u'), 265: ('h', 'v'), 266: ('h', 'w'), 267: ('h', 'x'), 268: ('h', 'y'), 269: ('h', 'z'), 270: ('i', '.'), 271: ('i', 'a'), 272: ('i', 'b'), 273: ('i', 'c'), 274: ('i', 'd'), 275: ('i', 'e'), 276: ('i', 'f'), 277: ('i', 'g'), 278: ('i', 'h'), 279: ('i', 'i'), 280: ('i', 'j'), 281: ('i', 'k'), 282: ('i', 'l'), 283: ('i', 'm'), 284: ('i', 'n'), 285: ('i', 'o'), 286: ('i', 'p'), 287: ('i', 'q'), 288: ('i', 'r'), 289: ('i', 's'), 290: ('i', 't'), 291: ('i', 'u'), 292: ('i', 'v'), 293: ('i', 'w'), 294: ('i', 'x'), 295: ('i', 'y'), 296: ('i', 'z'), 297: ('j', '.'), 298: ('j', 'a'), 299: ('j', 'b'), 300: ('j', 'c'), 301: ('j', 'd'), 302: ('j', 'e'), 303: ('j', 'f'), 304: ('j', 'g'), 305: ('j', 'h'), 306: ('j', 'i'), 307: ('j', 'j'), 308: ('j', 'k'), 309: ('j', 'l'), 310: ('j', 'm'), 311: ('j', 'n'), 312: ('j', 'o'), 313: ('j', 'p'), 314: ('j', 'q'), 315: ('j', 'r'), 316: ('j', 's'), 317: ('j', 't'), 318: ('j', 'u'), 319: ('j', 'v'), 320: ('j', 'w'), 321: ('j', 'x'), 322: ('j', 'y'), 323: ('j', 'z'), 324: ('k', '.'), 325: ('k', 'a'), 326: ('k', 'b'), 327: ('k', 'c'), 328: ('k', 'd'), 329: ('k', 'e'), 330: ('k', 'f'), 331: ('k', 'g'), 332: ('k', 'h'), 333: ('k', 'i'), 334: ('k', 'j'), 335: ('k', 'k'), 336: ('k', 'l'), 337: ('k', 'm'), 338: ('k', 'n'), 339: ('k', 'o'), 340: ('k', 'p'), 341: ('k', 'q'), 342: ('k', 'r'), 343: ('k', 's'), 344: ('k', 't'), 345: ('k', 'u'), 346: ('k', 'v'), 347: ('k', 'w'), 348: ('k', 'x'), 349: ('k', 'y'), 350: ('k', 'z'), 351: ('l', '.'), 352: ('l', 'a'), 353: ('l', 'b'), 354: ('l', 'c'), 355: ('l', 'd'), 356: ('l', 'e'), 357: ('l', 'f'), 358: ('l', 'g'), 359: ('l', 'h'), 360: ('l', 'i'), 361: ('l', 'j'), 362: ('l', 'k'), 363: ('l', 'l'), 364: ('l', 'm'), 365: ('l', 'n'), 366: ('l', 'o'), 367: ('l', 'p'), 368: ('l', 'q'), 369: ('l', 'r'), 370: ('l', 's'), 371: ('l', 't'), 372: ('l', 'u'), 373: ('l', 'v'), 374: ('l', 'w'), 375: ('l', 'x'), 376: ('l', 'y'), 377: ('l', 'z'), 378: ('m', '.'), 379: ('m', 'a'), 380: ('m', 'b'), 381: ('m', 'c'), 382: ('m', 'd'), 383: ('m', 'e'), 384: ('m', 'f'), 385: ('m', 'g'), 386: ('m', 'h'), 387: ('m', 'i'), 388: ('m', 'j'), 389: ('m', 'k'), 390: ('m', 'l'), 391: ('m', 'm'), 392: ('m', 'n'), 393: ('m', 'o'), 394: ('m', 'p'), 395: ('m', 'q'), 396: ('m', 'r'), 397: ('m', 's'), 398: ('m', 't'), 399: ('m', 'u'), 400: ('m', 'v'), 401: ('m', 'w'), 402: ('m', 'x'), 403: ('m', 'y'), 404: ('m', 'z'), 405: ('n', '.'), 406: ('n', 'a'), 407: ('n', 'b'), 408: ('n', 'c'), 409: ('n', 'd'), 410: ('n', 'e'), 411: ('n', 'f'), 412: ('n', 'g'), 413: ('n', 'h'), 414: ('n', 'i'), 415: ('n', 'j'), 416: ('n', 'k'), 417: ('n', 'l'), 418: ('n', 'm'), 419: ('n', 'n'), 420: ('n', 'o'), 421: ('n', 'p'), 422: ('n', 'q'), 423: ('n', 'r'), 424: ('n', 's'), 425: ('n', 't'), 426: ('n', 'u'), 427: ('n', 'v'), 428: ('n', 'w'), 429: ('n', 'x'), 430: ('n', 'y'), 431: ('n', 'z'), 432: ('o', '.'), 433: ('o', 'a'), 434: ('o', 'b'), 435: ('o', 'c'), 436: ('o', 'd'), 437: ('o', 'e'), 438: ('o', 'f'), 439: ('o', 'g'), 440: ('o', 'h'), 441: ('o', 'i'), 442: ('o', 'j'), 443: ('o', 'k'), 444: ('o', 'l'), 445: ('o', 'm'), 446: ('o', 'n'), 447: ('o', 'o'), 448: ('o', 'p'), 449: ('o', 'q'), 450: ('o', 'r'), 451: ('o', 's'), 452: ('o', 't'), 453: ('o', 'u'), 454: ('o', 'v'), 455: ('o', 'w'), 456: ('o', 'x'), 457: ('o', 'y'), 458: ('o', 'z'), 459: ('p', '.'), 460: ('p', 'a'), 461: ('p', 'b'), 462: ('p', 'c'), 463: ('p', 'd'), 464: ('p', 'e'), 465: ('p', 'f'), 466: ('p', 'g'), 467: ('p', 'h'), 468: ('p', 'i'), 469: ('p', 'j'), 470: ('p', 'k'), 471: ('p', 'l'), 472: ('p', 'm'), 473: ('p', 'n'), 474: ('p', 'o'), 475: ('p', 'p'), 476: ('p', 'q'), 477: ('p', 'r'), 478: ('p', 's'), 479: ('p', 't'), 480: ('p', 'u'), 481: ('p', 'v'), 482: ('p', 'w'), 483: ('p', 'x'), 484: ('p', 'y'), 485: ('p', 'z'), 486: ('q', '.'), 487: ('q', 'a'), 488: ('q', 'b'), 489: ('q', 'c'), 490: ('q', 'd'), 491: ('q', 'e'), 492: ('q', 'f'), 493: ('q', 'g'), 494: ('q', 'h'), 495: ('q', 'i'), 496: ('q', 'j'), 497: ('q', 'k'), 498: ('q', 'l'), 499: ('q', 'm'), 500: ('q', 'n'), 501: ('q', 'o'), 502: ('q', 'p'), 503: ('q', 'q'), 504: ('q', 'r'), 505: ('q', 's'), 506: ('q', 't'), 507: ('q', 'u'), 508: ('q', 'v'), 509: ('q', 'w'), 510: ('q', 'x'), 511: ('q', 'y'), 512: ('q', 'z'), 513: ('r', '.'), 514: ('r', 'a'), 515: ('r', 'b'), 516: ('r', 'c'), 517: ('r', 'd'), 518: ('r', 'e'), 519: ('r', 'f'), 520: ('r', 'g'), 521: ('r', 'h'), 522: ('r', 'i'), 523: ('r', 'j'), 524: ('r', 'k'), 525: ('r', 'l'), 526: ('r', 'm'), 527: ('r', 'n'), 528: ('r', 'o'), 529: ('r', 'p'), 530: ('r', 'q'), 531: ('r', 'r'), 532: ('r', 's'), 533: ('r', 't'), 534: ('r', 'u'), 535: ('r', 'v'), 536: ('r', 'w'), 537: ('r', 'x'), 538: ('r', 'y'), 539: ('r', 'z'), 540: ('s', '.'), 541: ('s', 'a'), 542: ('s', 'b'), 543: ('s', 'c'), 544: ('s', 'd'), 545: ('s', 'e'), 546: ('s', 'f'), 547: ('s', 'g'), 548: ('s', 'h'), 549: ('s', 'i'), 550: ('s', 'j'), 551: ('s', 'k'), 552: ('s', 'l'), 553: ('s', 'm'), 554: ('s', 'n'), 555: ('s', 'o'), 556: ('s', 'p'), 557: ('s', 'q'), 558: ('s', 'r'), 559: ('s', 's'), 560: ('s', 't'), 561: ('s', 'u'), 562: ('s', 'v'), 563: ('s', 'w'), 564: ('s', 'x'), 565: ('s', 'y'), 566: ('s', 'z'), 567: ('t', '.'), 568: ('t', 'a'), 569: ('t', 'b'), 570: ('t', 'c'), 571: ('t', 'd'), 572: ('t', 'e'), 573: ('t', 'f'), 574: ('t', 'g'), 575: ('t', 'h'), 576: ('t', 'i'), 577: ('t', 'j'), 578: ('t', 'k'), 579: ('t', 'l'), 580: ('t', 'm'), 581: ('t', 'n'), 582: ('t', 'o'), 583: ('t', 'p'), 584: ('t', 'q'), 585: ('t', 'r'), 586: ('t', 's'), 587: ('t', 't'), 588: ('t', 'u'), 589: ('t', 'v'), 590: ('t', 'w'), 591: ('t', 'x'), 592: ('t', 'y'), 593: ('t', 'z'), 594: ('u', '.'), 595: ('u', 'a'), 596: ('u', 'b'), 597: ('u', 'c'), 598: ('u', 'd'), 599: ('u', 'e'), 600: ('u', 'f'), 601: ('u', 'g'), 602: ('u', 'h'), 603: ('u', 'i'), 604: ('u', 'j'), 605: ('u', 'k'), 606: ('u', 'l'), 607: ('u', 'm'), 608: ('u', 'n'), 609: ('u', 'o'), 610: ('u', 'p'), 611: ('u', 'q'), 612: ('u', 'r'), 613: ('u', 's'), 614: ('u', 't'), 615: ('u', 'u'), 616: ('u', 'v'), 617: ('u', 'w'), 618: ('u', 'x'), 619: ('u', 'y'), 620: ('u', 'z'), 621: ('v', '.'), 622: ('v', 'a'), 623: ('v', 'b'), 624: ('v', 'c'), 625: ('v', 'd'), 626: ('v', 'e'), 627: ('v', 'f'), 628: ('v', 'g'), 629: ('v', 'h'), 630: ('v', 'i'), 631: ('v', 'j'), 632: ('v', 'k'), 633: ('v', 'l'), 634: ('v', 'm'), 635: ('v', 'n'), 636: ('v', 'o'), 637: ('v', 'p'), 638: ('v', 'q'), 639: ('v', 'r'), 640: ('v', 's'), 641: ('v', 't'), 642: ('v', 'u'), 643: ('v', 'v'), 644: ('v', 'w'), 645: ('v', 'x'), 646: ('v', 'y'), 647: ('v', 'z'), 648: ('w', '.'), 649: ('w', 'a'), 650: ('w', 'b'), 651: ('w', 'c'), 652: ('w', 'd'), 653: ('w', 'e'), 654: ('w', 'f'), 655: ('w', 'g'), 656: ('w', 'h'), 657: ('w', 'i'), 658: ('w', 'j'), 659: ('w', 'k'), 660: ('w', 'l'), 661: ('w', 'm'), 662: ('w', 'n'), 663: ('w', 'o'), 664: ('w', 'p'), 665: ('w', 'q'), 666: ('w', 'r'), 667: ('w', 's'), 668: ('w', 't'), 669: ('w', 'u'), 670: ('w', 'v'), 671: ('w', 'w'), 672: ('w', 'x'), 673: ('w', 'y'), 674: ('w', 'z'), 675: ('x', '.'), 676: ('x', 'a'), 677: ('x', 'b'), 678: ('x', 'c'), 679: ('x', 'd'), 680: ('x', 'e'), 681: ('x', 'f'), 682: ('x', 'g'), 683: ('x', 'h'), 684: ('x', 'i'), 685: ('x', 'j'), 686: ('x', 'k'), 687: ('x', 'l'), 688: ('x', 'm'), 689: ('x', 'n'), 690: ('x', 'o'), 691: ('x', 'p'), 692: ('x', 'q'), 693: ('x', 'r'), 694: ('x', 's'), 695: ('x', 't'), 696: ('x', 'u'), 697: ('x', 'v'), 698: ('x', 'w'), 699: ('x', 'x'), 700: ('x', 'y'), 701: ('x', 'z'), 702: ('y', '.'), 703: ('y', 'a'), 704: ('y', 'b'), 705: ('y', 'c'), 706: ('y', 'd'), 707: ('y', 'e'), 708: ('y', 'f'), 709: ('y', 'g'), 710: ('y', 'h'), 711: ('y', 'i'), 712: ('y', 'j'), 713: ('y', 'k'), 714: ('y', 'l'), 715: ('y', 'm'), 716: ('y', 'n'), 717: ('y', 'o'), 718: ('y', 'p'), 719: ('y', 'q'), 720: ('y', 'r'), 721: ('y', 's'), 722: ('y', 't'), 723: ('y', 'u'), 724: ('y', 'v'), 725: ('y', 'w'), 726: ('y', 'x'), 727: ('y', 'y'), 728: ('y', 'z'), 729: ('z', '.'), 730: ('z', 'a'), 731: ('z', 'b'), 732: ('z', 'c'), 733: ('z', 'd'), 734: ('z', 'e'), 735: ('z', 'f'), 736: ('z', 'g'), 737: ('z', 'h'), 738: ('z', 'i'), 739: ('z', 'j'), 740: ('z', 'k'), 741: ('z', 'l'), 742: ('z', 'm'), 743: ('z', 'n'), 744: ('z', 'o'), 745: ('z', 'p'), 746: ('z', 'q'), 747: ('z', 'r'), 748: ('z', 's'), 749: ('z', 't'), 750: ('z', 'u'), 751: ('z', 'v'), 752: ('z', 'w'), 753: ('z', 'x'), 754: ('z', 'y'), 755: ('z', 'z')}\n"
     ]
    }
   ],
   "source": [
    "stoi = {}\n",
    "idx = 0\n",
    "for i in range(len(chars)):\n",
    "    stoi[chars[i]] = idx \n",
    "    idx+=1\n",
    "for char_1 in chars:\n",
    "    for char_2 in chars:\n",
    "        stoi[(char_1, char_2)] = idx\n",
    "        idx  +=1\n",
    "\n",
    "itos = {i:c for c,i in stoi.items()}\n",
    "print(stoi)\n",
    "\n",
    "print(itos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "fa25d03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "trigram_tensor = torch.zeros((812,28), dtype=torch.int32)\n",
    "\n",
    "for name in names:\n",
    "    chrs = ['.', '.'] + list(name) + ['.']\n",
    "\n",
    "    for i in range(len(chrs) - 2):\n",
    "        trigram_tensor[stoi[(chrs[i], chrs[i+1])], stoi[chrs[i+2]]] += 1\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "17a45bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_tensor = trigram_tensor.float()\n",
    "trigram_tensor /=  trigram_tensor.sum(dim = 1, keepdim =  True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37700940",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "cd959808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oaqwentiliela\n",
      "azire\n",
      "kain\n",
      "bissayosa\n",
      "avy\n",
      "saamichu\n",
      "keidgen\n",
      "emi\n",
      "taihamesulia\n",
      "sanne\n",
      "naulietren\n",
      "na\n",
      "yah\n",
      "da\n",
      "em\n",
      "kariscoria\n",
      "kasiazhaa\n",
      "maralet\n",
      "amord\n",
      "ellahie\n",
      "carie\n",
      "lya\n",
      "rogio\n",
      "adain\n",
      "precius\n",
      "jus\n",
      "sakolby\n",
      "len\n",
      "jadamana\n",
      "zubrian\n",
      "xyon\n",
      "amir\n",
      "annajoua\n",
      "hamona\n",
      "tarjul\n",
      "elievalayah\n",
      "nash\n",
      "bett\n",
      "gran\n",
      "crakeyshundanna\n",
      "dia\n",
      "amersyri\n",
      "codierieliann\n",
      "shaelzaya\n",
      "ly\n",
      "ix\n",
      "race\n",
      "aleishubri\n",
      "aaleana\n",
      "fark\n",
      "dee\n",
      "brya\n",
      "aslin\n",
      "el\n",
      "renni\n",
      "hia\n",
      "novorane\n",
      "mice\n",
      "kin\n",
      "keightlyndavici\n",
      "fair\n",
      "elamir\n",
      "mer\n",
      "zakah\n",
      "nalmara\n",
      "arleigh\n",
      "que\n",
      "dra\n",
      "rajookeiddhwan\n",
      "ros\n",
      "safina\n",
      "ledis\n",
      "azion\n",
      "akal\n",
      "ivis\n",
      "an\n",
      "karri\n",
      "ki\n",
      "vinoramiraniamira\n",
      "ston\n",
      "oldeen\n",
      "havin\n",
      "jah\n",
      "zymissamarlidyanimokuntzavyn\n",
      "ny\n",
      "gleya\n",
      "srenesleth\n",
      "macia\n",
      "hangs\n",
      "im\n",
      "yah\n",
      "siandriseo\n",
      "omyah\n",
      "arlo\n",
      "de\n",
      "zaynn\n",
      "nyrie\n",
      "tinlee\n",
      "jah\n",
      "aneyas\n",
      "roserleydaleen\n",
      "li\n",
      "dah\n",
      "reighaun\n",
      "yux\n",
      "henn\n",
      "ke\n",
      "kae\n",
      "bret\n",
      "de\n",
      "aylasmili\n",
      "trilow\n",
      "han\n",
      "erres\n",
      "deani\n",
      "mehashi\n",
      "denna\n",
      "pelari\n",
      "ha\n",
      "jamerwyn\n",
      "da\n",
      "jayahmarseno\n",
      "cy\n",
      "arraxeni\n",
      "laiya\n",
      "no\n",
      "ty\n",
      "aree\n",
      "talidhyron\n",
      "ann\n",
      "te\n",
      "ivoron\n",
      "in\n",
      "kori\n",
      "adia\n",
      "ganne\n",
      "ron\n",
      "jhes\n",
      "aden\n",
      "za\n",
      "la\n",
      "chmicksi\n",
      "rin\n",
      "chail\n",
      "ty\n",
      "ham\n",
      "kairozeryssa\n",
      "clyn\n",
      "tai\n",
      "kam\n",
      "sari\n",
      "sa\n",
      "arhyra\n",
      "kimeel\n",
      "deya\n",
      "firasis\n",
      "alin\n",
      "gandiden\n",
      "aundomanden\n",
      "avi\n",
      "anno\n",
      "ion\n",
      "na\n",
      "arla\n",
      "rys\n",
      "lianna\n",
      "mario\n",
      "veddevanari\n",
      "bel\n",
      "mirking\n",
      "tynezliyontan\n",
      "mali\n",
      "beth\n",
      "kasuzurna\n",
      "ly\n",
      "ya\n",
      "chyelli\n",
      "kamie\n",
      "kermey\n",
      "josmana\n",
      "torandrenrigh\n",
      "berrosaalyn\n",
      "lanna\n",
      "styserav\n",
      "cadic\n",
      "ar\n",
      "bri\n",
      "marborry\n",
      "gielexith\n",
      "adia\n",
      "adriai\n",
      "manni\n",
      "dalisah\n",
      "khanithriahmiliellanogyn\n",
      "marquela\n",
      "er\n",
      "mutinever\n",
      "dana\n",
      "sam\n",
      "nix\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def sample_from_trigram(trigram_tensor):\n",
    "    \"\"\"\n",
    "    implementation of sampling a value from cumulative probability distribution\n",
    "    \"\"\"\n",
    "    cdf_tensor = trigram_tensor.cumsum(0)\n",
    "    rand_value = torch.rand(1)\n",
    "    idx = 0\n",
    "    for i in range(cdf_tensor.shape[0]):\n",
    "        if cdf_tensor[i] > rand_value:\n",
    "            idx = i\n",
    "            break\n",
    "\n",
    "    return idx\n",
    "\n",
    "for i in range(200):\n",
    "    idx = stoi[('.', '.')]\n",
    "    word = ['.', '.']\n",
    "    while True:\n",
    "        next_idx = sample_from_trigram(trigram_tensor[idx, :])\n",
    "        next_char = itos[next_idx]\n",
    "\n",
    "        word.append(next_char)\n",
    "\n",
    "        idx = stoi[(word[-2], word[-1])]\n",
    "        if next_idx == 0:\n",
    "            break\n",
    "    print(\"\".join(word[2:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "1261c701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'.': 0, 'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26}\n",
      "{0: '.', 1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z'}\n"
     ]
    }
   ],
   "source": [
    "chars = set()\n",
    "with open(\"names.txt\", \"r\") as f:\n",
    "    names = f.read().splitlines()\n",
    "for name in names:\n",
    "    chars.update(list(name))\n",
    "chars = sorted(chars)\n",
    "chars = ['.'] + list(chars) \n",
    "stoi = {}\n",
    "idx = 0\n",
    "for i in range(len(chars)):\n",
    "    stoi[chars[i]] = idx \n",
    "    idx+=1\n",
    "\n",
    "itos = {i:c for c,i in stoi.items()}\n",
    "print(stoi)\n",
    "\n",
    "print(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "d1814f18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([228146, 27])\n",
      "torch.Size([228146])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch\n",
    "x_feats = []\n",
    "y_feats = []\n",
    "\n",
    "\n",
    "for name in names:\n",
    "    chrs = ['.'] + list(name) + ['.']\n",
    "    for i in range(len(chrs) - 1):\n",
    "        x_idx = torch.tensor(stoi[chrs[i]])\n",
    "        y_idx = torch.tensor(stoi[chrs[i+1]])\n",
    "\n",
    "        x_feats.append(x_idx)\n",
    "        y_feats.append(y_idx)\n",
    "        # print(x_feat, y_feat)\n",
    "        # print(chrs[i], chrs[i+1])\n",
    "x_feats = torch.tensor(x_feats)\n",
    "y_feats = torch.tensor(y_feats)\n",
    "\n",
    "x_feats = F.one_hot(x_feats, num_classes=len(chars))\n",
    "\n",
    "x_feats = x_feats.float()\n",
    "\n",
    "print(x_feats.shape)\n",
    "print(y_feats.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "dab5af18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([228146, 27])\n"
     ]
    }
   ],
   "source": [
    "# x_feats = (228146, 27)\n",
    "# W = (27, 27)\n",
    "# X @ W = (228146, 27) @ (27, 27) = (228146, 27)\n",
    "W =  torch.randn((27, 27), requires_grad=True)\n",
    "O = x_feats @ W\n",
    "print(O.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "8e93a95f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.5710, -0.7529,  1.2221, -0.8897,  0.4779,  1.5621, -2.6768,  1.6389,\n",
      "        -0.3850, -0.5993, -0.8206, -0.7365,  0.5799,  0.2100, -0.5465, -0.7475,\n",
      "         0.5449,  0.4951, -2.2089, -1.8854,  0.1375, -0.2478,  0.3584,  1.6314,\n",
      "         1.2320,  2.6693,  0.0539], grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(O[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "593cb962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.5710, -0.7529,  1.2221, -0.8897,  0.4779,  1.5621, -2.6768,  1.6389,\n",
      "        -0.3850, -0.5993, -0.8206, -0.7365,  0.5799,  0.2100, -0.5465, -0.7475,\n",
      "         0.5449,  0.4951, -2.2089, -1.8854,  0.1375, -0.2478,  0.3584,  1.6314,\n",
      "         1.2320,  2.6693,  0.0539], grad_fn=<SliceBackward0>)\n",
      "tensor([54.8763], grad_fn=<SliceBackward0>)\n",
      "torch.Size([228146, 27])\n",
      "tensor([0.0323, 0.0086, 0.0619, 0.0075, 0.0294, 0.0869, 0.0013, 0.0938, 0.0124,\n",
      "        0.0100, 0.0080, 0.0087, 0.0325, 0.0225, 0.0106, 0.0086, 0.0314, 0.0299,\n",
      "        0.0020, 0.0028, 0.0209, 0.0142, 0.0261, 0.0931, 0.0625, 0.2629, 0.0192],\n",
      "       grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(O[0,:])\n",
    "O = O.exp()\n",
    "O_sum = O.sum(1, keepdim=True)\n",
    "O_softmax = O / O_sum\n",
    "print(O_sum[0,:])\n",
    "print(O_softmax.shape)\n",
    "print(O_softmax[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "ba67997b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([228146, 27])\n"
     ]
    }
   ],
   "source": [
    "print(O_softmax.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "fff006e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = O_softmax[ torch.arange(O_softmax.shape[0]), y_feats]\n",
    "loss = -loss.log().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee28a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.4900, grad_fn=<AddBackward0>)\n",
      "tensor(2.4900, grad_fn=<AddBackward0>)\n",
      "tensor(2.4900, grad_fn=<AddBackward0>)\n",
      "tensor(2.4899, grad_fn=<AddBackward0>)\n",
      "tensor(2.4899, grad_fn=<AddBackward0>)\n",
      "tensor(2.4899, grad_fn=<AddBackward0>)\n",
      "tensor(2.4898, grad_fn=<AddBackward0>)\n",
      "tensor(2.4898, grad_fn=<AddBackward0>)\n",
      "tensor(2.4898, grad_fn=<AddBackward0>)\n",
      "tensor(2.4898, grad_fn=<AddBackward0>)\n",
      "tensor(2.4897, grad_fn=<AddBackward0>)\n",
      "tensor(2.4897, grad_fn=<AddBackward0>)\n",
      "tensor(2.4897, grad_fn=<AddBackward0>)\n",
      "tensor(2.4896, grad_fn=<AddBackward0>)\n",
      "tensor(2.4896, grad_fn=<AddBackward0>)\n",
      "tensor(2.4896, grad_fn=<AddBackward0>)\n",
      "tensor(2.4895, grad_fn=<AddBackward0>)\n",
      "tensor(2.4895, grad_fn=<AddBackward0>)\n",
      "tensor(2.4895, grad_fn=<AddBackward0>)\n",
      "tensor(2.4895, grad_fn=<AddBackward0>)\n",
      "tensor(2.4894, grad_fn=<AddBackward0>)\n",
      "tensor(2.4894, grad_fn=<AddBackward0>)\n",
      "tensor(2.4894, grad_fn=<AddBackward0>)\n",
      "tensor(2.4893, grad_fn=<AddBackward0>)\n",
      "tensor(2.4893, grad_fn=<AddBackward0>)\n",
      "tensor(2.4893, grad_fn=<AddBackward0>)\n",
      "tensor(2.4893, grad_fn=<AddBackward0>)\n",
      "tensor(2.4892, grad_fn=<AddBackward0>)\n",
      "tensor(2.4892, grad_fn=<AddBackward0>)\n",
      "tensor(2.4892, grad_fn=<AddBackward0>)\n",
      "tensor(2.4891, grad_fn=<AddBackward0>)\n",
      "tensor(2.4891, grad_fn=<AddBackward0>)\n",
      "tensor(2.4891, grad_fn=<AddBackward0>)\n",
      "tensor(2.4891, grad_fn=<AddBackward0>)\n",
      "tensor(2.4890, grad_fn=<AddBackward0>)\n",
      "tensor(2.4890, grad_fn=<AddBackward0>)\n",
      "tensor(2.4890, grad_fn=<AddBackward0>)\n",
      "tensor(2.4890, grad_fn=<AddBackward0>)\n",
      "tensor(2.4889, grad_fn=<AddBackward0>)\n",
      "tensor(2.4889, grad_fn=<AddBackward0>)\n",
      "tensor(2.4889, grad_fn=<AddBackward0>)\n",
      "tensor(2.4888, grad_fn=<AddBackward0>)\n",
      "tensor(2.4888, grad_fn=<AddBackward0>)\n",
      "tensor(2.4888, grad_fn=<AddBackward0>)\n",
      "tensor(2.4888, grad_fn=<AddBackward0>)\n",
      "tensor(2.4887, grad_fn=<AddBackward0>)\n",
      "tensor(2.4887, grad_fn=<AddBackward0>)\n",
      "tensor(2.4887, grad_fn=<AddBackward0>)\n",
      "tensor(2.4887, grad_fn=<AddBackward0>)\n",
      "tensor(2.4886, grad_fn=<AddBackward0>)\n",
      "tensor(2.4886, grad_fn=<AddBackward0>)\n",
      "tensor(2.4886, grad_fn=<AddBackward0>)\n",
      "tensor(2.4886, grad_fn=<AddBackward0>)\n",
      "tensor(2.4885, grad_fn=<AddBackward0>)\n",
      "tensor(2.4885, grad_fn=<AddBackward0>)\n",
      "tensor(2.4885, grad_fn=<AddBackward0>)\n",
      "tensor(2.4885, grad_fn=<AddBackward0>)\n",
      "tensor(2.4884, grad_fn=<AddBackward0>)\n",
      "tensor(2.4884, grad_fn=<AddBackward0>)\n",
      "tensor(2.4884, grad_fn=<AddBackward0>)\n",
      "tensor(2.4884, grad_fn=<AddBackward0>)\n",
      "tensor(2.4883, grad_fn=<AddBackward0>)\n",
      "tensor(2.4883, grad_fn=<AddBackward0>)\n",
      "tensor(2.4883, grad_fn=<AddBackward0>)\n",
      "tensor(2.4883, grad_fn=<AddBackward0>)\n",
      "tensor(2.4882, grad_fn=<AddBackward0>)\n",
      "tensor(2.4882, grad_fn=<AddBackward0>)\n",
      "tensor(2.4882, grad_fn=<AddBackward0>)\n",
      "tensor(2.4882, grad_fn=<AddBackward0>)\n",
      "tensor(2.4882, grad_fn=<AddBackward0>)\n",
      "tensor(2.4881, grad_fn=<AddBackward0>)\n",
      "tensor(2.4881, grad_fn=<AddBackward0>)\n",
      "tensor(2.4881, grad_fn=<AddBackward0>)\n",
      "tensor(2.4881, grad_fn=<AddBackward0>)\n",
      "tensor(2.4880, grad_fn=<AddBackward0>)\n",
      "tensor(2.4880, grad_fn=<AddBackward0>)\n",
      "tensor(2.4880, grad_fn=<AddBackward0>)\n",
      "tensor(2.4880, grad_fn=<AddBackward0>)\n",
      "tensor(2.4880, grad_fn=<AddBackward0>)\n",
      "tensor(2.4879, grad_fn=<AddBackward0>)\n",
      "tensor(2.4879, grad_fn=<AddBackward0>)\n",
      "tensor(2.4879, grad_fn=<AddBackward0>)\n",
      "tensor(2.4879, grad_fn=<AddBackward0>)\n",
      "tensor(2.4878, grad_fn=<AddBackward0>)\n",
      "tensor(2.4878, grad_fn=<AddBackward0>)\n",
      "tensor(2.4878, grad_fn=<AddBackward0>)\n",
      "tensor(2.4878, grad_fn=<AddBackward0>)\n",
      "tensor(2.4878, grad_fn=<AddBackward0>)\n",
      "tensor(2.4877, grad_fn=<AddBackward0>)\n",
      "tensor(2.4877, grad_fn=<AddBackward0>)\n",
      "tensor(2.4877, grad_fn=<AddBackward0>)\n",
      "tensor(2.4877, grad_fn=<AddBackward0>)\n",
      "tensor(2.4876, grad_fn=<AddBackward0>)\n",
      "tensor(2.4876, grad_fn=<AddBackward0>)\n",
      "tensor(2.4876, grad_fn=<AddBackward0>)\n",
      "tensor(2.4876, grad_fn=<AddBackward0>)\n",
      "tensor(2.4876, grad_fn=<AddBackward0>)\n",
      "tensor(2.4875, grad_fn=<AddBackward0>)\n",
      "tensor(2.4875, grad_fn=<AddBackward0>)\n",
      "tensor(2.4875, grad_fn=<AddBackward0>)\n",
      "tensor(2.4875, grad_fn=<AddBackward0>)\n",
      "tensor(2.4875, grad_fn=<AddBackward0>)\n",
      "tensor(2.4874, grad_fn=<AddBackward0>)\n",
      "tensor(2.4874, grad_fn=<AddBackward0>)\n",
      "tensor(2.4874, grad_fn=<AddBackward0>)\n",
      "tensor(2.4874, grad_fn=<AddBackward0>)\n",
      "tensor(2.4874, grad_fn=<AddBackward0>)\n",
      "tensor(2.4873, grad_fn=<AddBackward0>)\n",
      "tensor(2.4873, grad_fn=<AddBackward0>)\n",
      "tensor(2.4873, grad_fn=<AddBackward0>)\n",
      "tensor(2.4873, grad_fn=<AddBackward0>)\n",
      "tensor(2.4873, grad_fn=<AddBackward0>)\n",
      "tensor(2.4872, grad_fn=<AddBackward0>)\n",
      "tensor(2.4872, grad_fn=<AddBackward0>)\n",
      "tensor(2.4872, grad_fn=<AddBackward0>)\n",
      "tensor(2.4872, grad_fn=<AddBackward0>)\n",
      "tensor(2.4872, grad_fn=<AddBackward0>)\n",
      "tensor(2.4871, grad_fn=<AddBackward0>)\n",
      "tensor(2.4871, grad_fn=<AddBackward0>)\n",
      "tensor(2.4871, grad_fn=<AddBackward0>)\n",
      "tensor(2.4871, grad_fn=<AddBackward0>)\n",
      "tensor(2.4871, grad_fn=<AddBackward0>)\n",
      "tensor(2.4871, grad_fn=<AddBackward0>)\n",
      "tensor(2.4870, grad_fn=<AddBackward0>)\n",
      "tensor(2.4870, grad_fn=<AddBackward0>)\n",
      "tensor(2.4870, grad_fn=<AddBackward0>)\n",
      "tensor(2.4870, grad_fn=<AddBackward0>)\n",
      "tensor(2.4870, grad_fn=<AddBackward0>)\n",
      "tensor(2.4869, grad_fn=<AddBackward0>)\n",
      "tensor(2.4869, grad_fn=<AddBackward0>)\n",
      "tensor(2.4869, grad_fn=<AddBackward0>)\n",
      "tensor(2.4869, grad_fn=<AddBackward0>)\n",
      "tensor(2.4869, grad_fn=<AddBackward0>)\n",
      "tensor(2.4869, grad_fn=<AddBackward0>)\n",
      "tensor(2.4868, grad_fn=<AddBackward0>)\n",
      "tensor(2.4868, grad_fn=<AddBackward0>)\n",
      "tensor(2.4868, grad_fn=<AddBackward0>)\n",
      "tensor(2.4868, grad_fn=<AddBackward0>)\n",
      "tensor(2.4868, grad_fn=<AddBackward0>)\n",
      "tensor(2.4867, grad_fn=<AddBackward0>)\n",
      "tensor(2.4867, grad_fn=<AddBackward0>)\n",
      "tensor(2.4867, grad_fn=<AddBackward0>)\n",
      "tensor(2.4867, grad_fn=<AddBackward0>)\n",
      "tensor(2.4867, grad_fn=<AddBackward0>)\n",
      "tensor(2.4867, grad_fn=<AddBackward0>)\n",
      "tensor(2.4866, grad_fn=<AddBackward0>)\n",
      "tensor(2.4866, grad_fn=<AddBackward0>)\n",
      "tensor(2.4866, grad_fn=<AddBackward0>)\n",
      "tensor(2.4866, grad_fn=<AddBackward0>)\n",
      "tensor(2.4866, grad_fn=<AddBackward0>)\n",
      "tensor(2.4866, grad_fn=<AddBackward0>)\n",
      "tensor(2.4865, grad_fn=<AddBackward0>)\n",
      "tensor(2.4865, grad_fn=<AddBackward0>)\n",
      "tensor(2.4865, grad_fn=<AddBackward0>)\n",
      "tensor(2.4865, grad_fn=<AddBackward0>)\n",
      "tensor(2.4865, grad_fn=<AddBackward0>)\n",
      "tensor(2.4865, grad_fn=<AddBackward0>)\n",
      "tensor(2.4864, grad_fn=<AddBackward0>)\n",
      "tensor(2.4864, grad_fn=<AddBackward0>)\n",
      "tensor(2.4864, grad_fn=<AddBackward0>)\n",
      "tensor(2.4864, grad_fn=<AddBackward0>)\n",
      "tensor(2.4864, grad_fn=<AddBackward0>)\n",
      "tensor(2.4864, grad_fn=<AddBackward0>)\n",
      "tensor(2.4863, grad_fn=<AddBackward0>)\n",
      "tensor(2.4863, grad_fn=<AddBackward0>)\n",
      "tensor(2.4863, grad_fn=<AddBackward0>)\n",
      "tensor(2.4863, grad_fn=<AddBackward0>)\n",
      "tensor(2.4863, grad_fn=<AddBackward0>)\n",
      "tensor(2.4863, grad_fn=<AddBackward0>)\n",
      "tensor(2.4863, grad_fn=<AddBackward0>)\n",
      "tensor(2.4862, grad_fn=<AddBackward0>)\n",
      "tensor(2.4862, grad_fn=<AddBackward0>)\n",
      "tensor(2.4862, grad_fn=<AddBackward0>)\n",
      "tensor(2.4862, grad_fn=<AddBackward0>)\n",
      "tensor(2.4862, grad_fn=<AddBackward0>)\n",
      "tensor(2.4862, grad_fn=<AddBackward0>)\n",
      "tensor(2.4861, grad_fn=<AddBackward0>)\n",
      "tensor(2.4861, grad_fn=<AddBackward0>)\n",
      "tensor(2.4861, grad_fn=<AddBackward0>)\n",
      "tensor(2.4861, grad_fn=<AddBackward0>)\n",
      "tensor(2.4861, grad_fn=<AddBackward0>)\n",
      "tensor(2.4861, grad_fn=<AddBackward0>)\n",
      "tensor(2.4861, grad_fn=<AddBackward0>)\n",
      "tensor(2.4860, grad_fn=<AddBackward0>)\n",
      "tensor(2.4860, grad_fn=<AddBackward0>)\n",
      "tensor(2.4860, grad_fn=<AddBackward0>)\n",
      "tensor(2.4860, grad_fn=<AddBackward0>)\n",
      "tensor(2.4860, grad_fn=<AddBackward0>)\n",
      "tensor(2.4860, grad_fn=<AddBackward0>)\n",
      "tensor(2.4860, grad_fn=<AddBackward0>)\n",
      "tensor(2.4859, grad_fn=<AddBackward0>)\n",
      "tensor(2.4859, grad_fn=<AddBackward0>)\n",
      "tensor(2.4859, grad_fn=<AddBackward0>)\n",
      "tensor(2.4859, grad_fn=<AddBackward0>)\n",
      "tensor(2.4859, grad_fn=<AddBackward0>)\n",
      "tensor(2.4859, grad_fn=<AddBackward0>)\n",
      "tensor(2.4859, grad_fn=<AddBackward0>)\n",
      "tensor(2.4858, grad_fn=<AddBackward0>)\n",
      "tensor(2.4858, grad_fn=<AddBackward0>)\n",
      "tensor(2.4858, grad_fn=<AddBackward0>)\n",
      "tensor(2.4858, grad_fn=<AddBackward0>)\n",
      "tensor(2.4858, grad_fn=<AddBackward0>)\n",
      "tensor(2.4858, grad_fn=<AddBackward0>)\n",
      "tensor(2.4858, grad_fn=<AddBackward0>)\n",
      "tensor(2.4857, grad_fn=<AddBackward0>)\n",
      "tensor(2.4857, grad_fn=<AddBackward0>)\n",
      "tensor(2.4857, grad_fn=<AddBackward0>)\n",
      "tensor(2.4857, grad_fn=<AddBackward0>)\n",
      "tensor(2.4857, grad_fn=<AddBackward0>)\n",
      "tensor(2.4857, grad_fn=<AddBackward0>)\n",
      "tensor(2.4857, grad_fn=<AddBackward0>)\n",
      "tensor(2.4856, grad_fn=<AddBackward0>)\n",
      "tensor(2.4856, grad_fn=<AddBackward0>)\n",
      "tensor(2.4856, grad_fn=<AddBackward0>)\n",
      "tensor(2.4856, grad_fn=<AddBackward0>)\n",
      "tensor(2.4856, grad_fn=<AddBackward0>)\n",
      "tensor(2.4856, grad_fn=<AddBackward0>)\n",
      "tensor(2.4856, grad_fn=<AddBackward0>)\n",
      "tensor(2.4856, grad_fn=<AddBackward0>)\n",
      "tensor(2.4855, grad_fn=<AddBackward0>)\n",
      "tensor(2.4855, grad_fn=<AddBackward0>)\n",
      "tensor(2.4855, grad_fn=<AddBackward0>)\n",
      "tensor(2.4855, grad_fn=<AddBackward0>)\n",
      "tensor(2.4855, grad_fn=<AddBackward0>)\n",
      "tensor(2.4855, grad_fn=<AddBackward0>)\n",
      "tensor(2.4855, grad_fn=<AddBackward0>)\n",
      "tensor(2.4855, grad_fn=<AddBackward0>)\n",
      "tensor(2.4854, grad_fn=<AddBackward0>)\n",
      "tensor(2.4854, grad_fn=<AddBackward0>)\n",
      "tensor(2.4854, grad_fn=<AddBackward0>)\n",
      "tensor(2.4854, grad_fn=<AddBackward0>)\n",
      "tensor(2.4854, grad_fn=<AddBackward0>)\n",
      "tensor(2.4854, grad_fn=<AddBackward0>)\n",
      "tensor(2.4854, grad_fn=<AddBackward0>)\n",
      "tensor(2.4854, grad_fn=<AddBackward0>)\n",
      "tensor(2.4853, grad_fn=<AddBackward0>)\n",
      "tensor(2.4853, grad_fn=<AddBackward0>)\n",
      "tensor(2.4853, grad_fn=<AddBackward0>)\n",
      "tensor(2.4853, grad_fn=<AddBackward0>)\n",
      "tensor(2.4853, grad_fn=<AddBackward0>)\n",
      "tensor(2.4853, grad_fn=<AddBackward0>)\n",
      "tensor(2.4853, grad_fn=<AddBackward0>)\n",
      "tensor(2.4853, grad_fn=<AddBackward0>)\n",
      "tensor(2.4852, grad_fn=<AddBackward0>)\n",
      "tensor(2.4852, grad_fn=<AddBackward0>)\n",
      "tensor(2.4852, grad_fn=<AddBackward0>)\n",
      "tensor(2.4852, grad_fn=<AddBackward0>)\n",
      "tensor(2.4852, grad_fn=<AddBackward0>)\n",
      "tensor(2.4852, grad_fn=<AddBackward0>)\n",
      "tensor(2.4852, grad_fn=<AddBackward0>)\n",
      "tensor(2.4852, grad_fn=<AddBackward0>)\n",
      "tensor(2.4852, grad_fn=<AddBackward0>)\n",
      "tensor(2.4851, grad_fn=<AddBackward0>)\n",
      "tensor(2.4851, grad_fn=<AddBackward0>)\n",
      "tensor(2.4851, grad_fn=<AddBackward0>)\n",
      "tensor(2.4851, grad_fn=<AddBackward0>)\n",
      "tensor(2.4851, grad_fn=<AddBackward0>)\n",
      "tensor(2.4851, grad_fn=<AddBackward0>)\n",
      "tensor(2.4851, grad_fn=<AddBackward0>)\n",
      "tensor(2.4851, grad_fn=<AddBackward0>)\n",
      "tensor(2.4850, grad_fn=<AddBackward0>)\n",
      "tensor(2.4850, grad_fn=<AddBackward0>)\n",
      "tensor(2.4850, grad_fn=<AddBackward0>)\n",
      "tensor(2.4850, grad_fn=<AddBackward0>)\n",
      "tensor(2.4850, grad_fn=<AddBackward0>)\n",
      "tensor(2.4850, grad_fn=<AddBackward0>)\n",
      "tensor(2.4850, grad_fn=<AddBackward0>)\n",
      "tensor(2.4850, grad_fn=<AddBackward0>)\n",
      "tensor(2.4850, grad_fn=<AddBackward0>)\n",
      "tensor(2.4849, grad_fn=<AddBackward0>)\n",
      "tensor(2.4849, grad_fn=<AddBackward0>)\n",
      "tensor(2.4849, grad_fn=<AddBackward0>)\n",
      "tensor(2.4849, grad_fn=<AddBackward0>)\n",
      "tensor(2.4849, grad_fn=<AddBackward0>)\n",
      "tensor(2.4849, grad_fn=<AddBackward0>)\n",
      "tensor(2.4849, grad_fn=<AddBackward0>)\n",
      "tensor(2.4849, grad_fn=<AddBackward0>)\n",
      "tensor(2.4849, grad_fn=<AddBackward0>)\n",
      "tensor(2.4849, grad_fn=<AddBackward0>)\n",
      "tensor(2.4848, grad_fn=<AddBackward0>)\n",
      "tensor(2.4848, grad_fn=<AddBackward0>)\n",
      "tensor(2.4848, grad_fn=<AddBackward0>)\n",
      "tensor(2.4848, grad_fn=<AddBackward0>)\n",
      "tensor(2.4848, grad_fn=<AddBackward0>)\n",
      "tensor(2.4848, grad_fn=<AddBackward0>)\n",
      "tensor(2.4848, grad_fn=<AddBackward0>)\n",
      "tensor(2.4848, grad_fn=<AddBackward0>)\n",
      "tensor(2.4848, grad_fn=<AddBackward0>)\n",
      "tensor(2.4847, grad_fn=<AddBackward0>)\n",
      "tensor(2.4847, grad_fn=<AddBackward0>)\n",
      "tensor(2.4847, grad_fn=<AddBackward0>)\n",
      "tensor(2.4847, grad_fn=<AddBackward0>)\n",
      "tensor(2.4847, grad_fn=<AddBackward0>)\n",
      "tensor(2.4847, grad_fn=<AddBackward0>)\n",
      "tensor(2.4847, grad_fn=<AddBackward0>)\n",
      "tensor(2.4847, grad_fn=<AddBackward0>)\n",
      "tensor(2.4847, grad_fn=<AddBackward0>)\n",
      "tensor(2.4847, grad_fn=<AddBackward0>)\n",
      "tensor(2.4846, grad_fn=<AddBackward0>)\n",
      "tensor(2.4846, grad_fn=<AddBackward0>)\n",
      "tensor(2.4846, grad_fn=<AddBackward0>)\n",
      "tensor(2.4846, grad_fn=<AddBackward0>)\n",
      "tensor(2.4846, grad_fn=<AddBackward0>)\n",
      "tensor(2.4846, grad_fn=<AddBackward0>)\n",
      "tensor(2.4846, grad_fn=<AddBackward0>)\n",
      "tensor(2.4846, grad_fn=<AddBackward0>)\n",
      "tensor(2.4846, grad_fn=<AddBackward0>)\n",
      "tensor(2.4846, grad_fn=<AddBackward0>)\n",
      "tensor(2.4845, grad_fn=<AddBackward0>)\n",
      "tensor(2.4845, grad_fn=<AddBackward0>)\n",
      "tensor(2.4845, grad_fn=<AddBackward0>)\n",
      "tensor(2.4845, grad_fn=<AddBackward0>)\n",
      "tensor(2.4845, grad_fn=<AddBackward0>)\n",
      "tensor(2.4845, grad_fn=<AddBackward0>)\n",
      "tensor(2.4845, grad_fn=<AddBackward0>)\n",
      "tensor(2.4845, grad_fn=<AddBackward0>)\n",
      "tensor(2.4845, grad_fn=<AddBackward0>)\n",
      "tensor(2.4845, grad_fn=<AddBackward0>)\n",
      "tensor(2.4845, grad_fn=<AddBackward0>)\n",
      "tensor(2.4844, grad_fn=<AddBackward0>)\n",
      "tensor(2.4844, grad_fn=<AddBackward0>)\n",
      "tensor(2.4844, grad_fn=<AddBackward0>)\n",
      "tensor(2.4844, grad_fn=<AddBackward0>)\n",
      "tensor(2.4844, grad_fn=<AddBackward0>)\n",
      "tensor(2.4844, grad_fn=<AddBackward0>)\n",
      "tensor(2.4844, grad_fn=<AddBackward0>)\n",
      "tensor(2.4844, grad_fn=<AddBackward0>)\n",
      "tensor(2.4844, grad_fn=<AddBackward0>)\n",
      "tensor(2.4844, grad_fn=<AddBackward0>)\n",
      "tensor(2.4844, grad_fn=<AddBackward0>)\n",
      "tensor(2.4843, grad_fn=<AddBackward0>)\n",
      "tensor(2.4843, grad_fn=<AddBackward0>)\n",
      "tensor(2.4843, grad_fn=<AddBackward0>)\n",
      "tensor(2.4843, grad_fn=<AddBackward0>)\n",
      "tensor(2.4843, grad_fn=<AddBackward0>)\n",
      "tensor(2.4843, grad_fn=<AddBackward0>)\n",
      "tensor(2.4843, grad_fn=<AddBackward0>)\n",
      "tensor(2.4843, grad_fn=<AddBackward0>)\n",
      "tensor(2.4843, grad_fn=<AddBackward0>)\n",
      "tensor(2.4843, grad_fn=<AddBackward0>)\n",
      "tensor(2.4843, grad_fn=<AddBackward0>)\n",
      "tensor(2.4842, grad_fn=<AddBackward0>)\n",
      "tensor(2.4842, grad_fn=<AddBackward0>)\n",
      "tensor(2.4842, grad_fn=<AddBackward0>)\n",
      "tensor(2.4842, grad_fn=<AddBackward0>)\n",
      "tensor(2.4842, grad_fn=<AddBackward0>)\n",
      "tensor(2.4842, grad_fn=<AddBackward0>)\n",
      "tensor(2.4842, grad_fn=<AddBackward0>)\n",
      "tensor(2.4842, grad_fn=<AddBackward0>)\n",
      "tensor(2.4842, grad_fn=<AddBackward0>)\n",
      "tensor(2.4842, grad_fn=<AddBackward0>)\n",
      "tensor(2.4842, grad_fn=<AddBackward0>)\n",
      "tensor(2.4842, grad_fn=<AddBackward0>)\n",
      "tensor(2.4841, grad_fn=<AddBackward0>)\n",
      "tensor(2.4841, grad_fn=<AddBackward0>)\n",
      "tensor(2.4841, grad_fn=<AddBackward0>)\n",
      "tensor(2.4841, grad_fn=<AddBackward0>)\n",
      "tensor(2.4841, grad_fn=<AddBackward0>)\n",
      "tensor(2.4841, grad_fn=<AddBackward0>)\n",
      "tensor(2.4841, grad_fn=<AddBackward0>)\n",
      "tensor(2.4841, grad_fn=<AddBackward0>)\n",
      "tensor(2.4841, grad_fn=<AddBackward0>)\n",
      "tensor(2.4841, grad_fn=<AddBackward0>)\n",
      "tensor(2.4841, grad_fn=<AddBackward0>)\n",
      "tensor(2.4841, grad_fn=<AddBackward0>)\n",
      "tensor(2.4840, grad_fn=<AddBackward0>)\n",
      "tensor(2.4840, grad_fn=<AddBackward0>)\n",
      "tensor(2.4840, grad_fn=<AddBackward0>)\n",
      "tensor(2.4840, grad_fn=<AddBackward0>)\n",
      "tensor(2.4840, grad_fn=<AddBackward0>)\n",
      "tensor(2.4840, grad_fn=<AddBackward0>)\n",
      "tensor(2.4840, grad_fn=<AddBackward0>)\n",
      "tensor(2.4840, grad_fn=<AddBackward0>)\n",
      "tensor(2.4840, grad_fn=<AddBackward0>)\n",
      "tensor(2.4840, grad_fn=<AddBackward0>)\n",
      "tensor(2.4840, grad_fn=<AddBackward0>)\n",
      "tensor(2.4840, grad_fn=<AddBackward0>)\n",
      "tensor(2.4840, grad_fn=<AddBackward0>)\n",
      "tensor(2.4839, grad_fn=<AddBackward0>)\n",
      "tensor(2.4839, grad_fn=<AddBackward0>)\n",
      "tensor(2.4839, grad_fn=<AddBackward0>)\n",
      "tensor(2.4839, grad_fn=<AddBackward0>)\n",
      "tensor(2.4839, grad_fn=<AddBackward0>)\n",
      "tensor(2.4839, grad_fn=<AddBackward0>)\n",
      "tensor(2.4839, grad_fn=<AddBackward0>)\n",
      "tensor(2.4839, grad_fn=<AddBackward0>)\n",
      "tensor(2.4839, grad_fn=<AddBackward0>)\n",
      "tensor(2.4839, grad_fn=<AddBackward0>)\n",
      "tensor(2.4839, grad_fn=<AddBackward0>)\n",
      "tensor(2.4839, grad_fn=<AddBackward0>)\n",
      "tensor(2.4839, grad_fn=<AddBackward0>)\n",
      "tensor(2.4838, grad_fn=<AddBackward0>)\n",
      "tensor(2.4838, grad_fn=<AddBackward0>)\n",
      "tensor(2.4838, grad_fn=<AddBackward0>)\n",
      "tensor(2.4838, grad_fn=<AddBackward0>)\n",
      "tensor(2.4838, grad_fn=<AddBackward0>)\n",
      "tensor(2.4838, grad_fn=<AddBackward0>)\n",
      "tensor(2.4838, grad_fn=<AddBackward0>)\n",
      "tensor(2.4838, grad_fn=<AddBackward0>)\n",
      "tensor(2.4838, grad_fn=<AddBackward0>)\n",
      "tensor(2.4838, grad_fn=<AddBackward0>)\n",
      "tensor(2.4838, grad_fn=<AddBackward0>)\n",
      "tensor(2.4838, grad_fn=<AddBackward0>)\n",
      "tensor(2.4838, grad_fn=<AddBackward0>)\n",
      "tensor(2.4838, grad_fn=<AddBackward0>)\n",
      "tensor(2.4837, grad_fn=<AddBackward0>)\n",
      "tensor(2.4837, grad_fn=<AddBackward0>)\n",
      "tensor(2.4837, grad_fn=<AddBackward0>)\n",
      "tensor(2.4837, grad_fn=<AddBackward0>)\n",
      "tensor(2.4837, grad_fn=<AddBackward0>)\n",
      "tensor(2.4837, grad_fn=<AddBackward0>)\n",
      "tensor(2.4837, grad_fn=<AddBackward0>)\n",
      "tensor(2.4837, grad_fn=<AddBackward0>)\n",
      "tensor(2.4837, grad_fn=<AddBackward0>)\n",
      "tensor(2.4837, grad_fn=<AddBackward0>)\n",
      "tensor(2.4837, grad_fn=<AddBackward0>)\n",
      "tensor(2.4837, grad_fn=<AddBackward0>)\n",
      "tensor(2.4837, grad_fn=<AddBackward0>)\n",
      "tensor(2.4837, grad_fn=<AddBackward0>)\n",
      "tensor(2.4836, grad_fn=<AddBackward0>)\n",
      "tensor(2.4836, grad_fn=<AddBackward0>)\n",
      "tensor(2.4836, grad_fn=<AddBackward0>)\n",
      "tensor(2.4836, grad_fn=<AddBackward0>)\n",
      "tensor(2.4836, grad_fn=<AddBackward0>)\n",
      "tensor(2.4836, grad_fn=<AddBackward0>)\n",
      "tensor(2.4836, grad_fn=<AddBackward0>)\n",
      "tensor(2.4836, grad_fn=<AddBackward0>)\n",
      "tensor(2.4836, grad_fn=<AddBackward0>)\n",
      "tensor(2.4836, grad_fn=<AddBackward0>)\n",
      "tensor(2.4836, grad_fn=<AddBackward0>)\n",
      "tensor(2.4836, grad_fn=<AddBackward0>)\n",
      "tensor(2.4836, grad_fn=<AddBackward0>)\n",
      "tensor(2.4836, grad_fn=<AddBackward0>)\n",
      "tensor(2.4836, grad_fn=<AddBackward0>)\n",
      "tensor(2.4835, grad_fn=<AddBackward0>)\n",
      "tensor(2.4835, grad_fn=<AddBackward0>)\n",
      "tensor(2.4835, grad_fn=<AddBackward0>)\n",
      "tensor(2.4835, grad_fn=<AddBackward0>)\n",
      "tensor(2.4835, grad_fn=<AddBackward0>)\n",
      "tensor(2.4835, grad_fn=<AddBackward0>)\n",
      "tensor(2.4835, grad_fn=<AddBackward0>)\n",
      "tensor(2.4835, grad_fn=<AddBackward0>)\n",
      "tensor(2.4835, grad_fn=<AddBackward0>)\n",
      "tensor(2.4835, grad_fn=<AddBackward0>)\n",
      "tensor(2.4835, grad_fn=<AddBackward0>)\n",
      "tensor(2.4835, grad_fn=<AddBackward0>)\n",
      "tensor(2.4835, grad_fn=<AddBackward0>)\n",
      "tensor(2.4835, grad_fn=<AddBackward0>)\n",
      "tensor(2.4835, grad_fn=<AddBackward0>)\n",
      "tensor(2.4835, grad_fn=<AddBackward0>)\n",
      "tensor(2.4834, grad_fn=<AddBackward0>)\n",
      "tensor(2.4834, grad_fn=<AddBackward0>)\n",
      "tensor(2.4834, grad_fn=<AddBackward0>)\n",
      "tensor(2.4834, grad_fn=<AddBackward0>)\n",
      "tensor(2.4834, grad_fn=<AddBackward0>)\n",
      "tensor(2.4834, grad_fn=<AddBackward0>)\n",
      "tensor(2.4834, grad_fn=<AddBackward0>)\n",
      "tensor(2.4834, grad_fn=<AddBackward0>)\n",
      "tensor(2.4834, grad_fn=<AddBackward0>)\n",
      "tensor(2.4834, grad_fn=<AddBackward0>)\n",
      "tensor(2.4834, grad_fn=<AddBackward0>)\n",
      "tensor(2.4834, grad_fn=<AddBackward0>)\n",
      "tensor(2.4834, grad_fn=<AddBackward0>)\n",
      "tensor(2.4834, grad_fn=<AddBackward0>)\n",
      "tensor(2.4834, grad_fn=<AddBackward0>)\n",
      "tensor(2.4834, grad_fn=<AddBackward0>)\n",
      "tensor(2.4833, grad_fn=<AddBackward0>)\n",
      "tensor(2.4833, grad_fn=<AddBackward0>)\n",
      "tensor(2.4833, grad_fn=<AddBackward0>)\n",
      "tensor(2.4833, grad_fn=<AddBackward0>)\n",
      "tensor(2.4833, grad_fn=<AddBackward0>)\n",
      "tensor(2.4833, grad_fn=<AddBackward0>)\n",
      "tensor(2.4833, grad_fn=<AddBackward0>)\n",
      "tensor(2.4833, grad_fn=<AddBackward0>)\n",
      "tensor(2.4833, grad_fn=<AddBackward0>)\n",
      "tensor(2.4833, grad_fn=<AddBackward0>)\n",
      "tensor(2.4833, grad_fn=<AddBackward0>)\n",
      "tensor(2.4833, grad_fn=<AddBackward0>)\n",
      "tensor(2.4833, grad_fn=<AddBackward0>)\n",
      "tensor(2.4833, grad_fn=<AddBackward0>)\n",
      "tensor(2.4833, grad_fn=<AddBackward0>)\n",
      "tensor(2.4833, grad_fn=<AddBackward0>)\n",
      "tensor(2.4833, grad_fn=<AddBackward0>)\n",
      "tensor(2.4833, grad_fn=<AddBackward0>)\n",
      "tensor(2.4832, grad_fn=<AddBackward0>)\n",
      "tensor(2.4832, grad_fn=<AddBackward0>)\n",
      "tensor(2.4832, grad_fn=<AddBackward0>)\n",
      "tensor(2.4832, grad_fn=<AddBackward0>)\n",
      "tensor(2.4832, grad_fn=<AddBackward0>)\n",
      "tensor(2.4832, grad_fn=<AddBackward0>)\n",
      "tensor(2.4832, grad_fn=<AddBackward0>)\n",
      "tensor(2.4832, grad_fn=<AddBackward0>)\n",
      "tensor(2.4832, grad_fn=<AddBackward0>)\n",
      "tensor(2.4832, grad_fn=<AddBackward0>)\n",
      "tensor(2.4832, grad_fn=<AddBackward0>)\n",
      "tensor(2.4832, grad_fn=<AddBackward0>)\n",
      "tensor(2.4832, grad_fn=<AddBackward0>)\n",
      "tensor(2.4832, grad_fn=<AddBackward0>)\n",
      "tensor(2.4832, grad_fn=<AddBackward0>)\n",
      "tensor(2.4832, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for i in range(500):\n",
    "    O = x_feats @ W\n",
    "    O = O.exp()\n",
    "    O_sum = O.sum(1, keepdim=True)\n",
    "    O_softmax = O / O_sum\n",
    "    loss = O_softmax[ torch.arange(O_softmax.shape[0]), y_feats]\n",
    "    loss = -loss.log().mean() + 0.01 * (W ** 2).mean() # adding regularization is same as adding a small initial value to bigram frequency matrix, it makes the model smoother\n",
    "    loss.backward() # this only calculates the gradients, yu need to manually do w.data -= 10 * W.grad in order to update the weights \n",
    "    W.data -= 10 * W.grad\n",
    "    W.grad.zero_() # this is important, otherwise the gradients will accumulate\n",
    "    print(loss)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "b63622da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "leock.\n",
      "kilasanesh.\n",
      "lelices.\n",
      "emad.\n",
      "ah.\n",
      "jaylulust.\n",
      "jaa.\n",
      "jelerys.\n",
      "prelirayahallolevigeydrh.\n",
      "tes.\n",
      "amaleya.\n",
      "rendnah.\n",
      "mashahw.\n",
      "rielynorliahaccaren.\n",
      "k.\n",
      "jak.\n",
      "jamara.\n",
      "tviesajeenahauutfn.\n",
      "jan.\n",
      "ciannnasticisanakamesh.\n",
      "rlaserren.\n",
      "e.\n",
      "vitrielavind.\n",
      "brahniatan.\n",
      "aahva.\n",
      "haly.\n",
      "endarsapverisleaky.\n",
      "zabih.\n",
      "la.\n",
      "m.\n",
      "maa.\n",
      "dryvoanahn.\n",
      "anighayan.\n",
      "ali.\n",
      "jureetaizahathaynpeedilesumeigebasibolllorn.\n",
      "lavkallah.\n",
      "son.\n",
      "rayarran.\n",
      "jixsa.\n",
      "dr.\n",
      "an.\n",
      "en.\n",
      "dzamabrettlilyitoje.\n",
      "yanonajain.\n",
      "jxzm.\n",
      "tre.\n",
      "elllelinyn.\n",
      "apach.\n",
      "ga.\n",
      "maniashi.\n",
      "coxdronylatetooolilil.\n",
      "abr.\n",
      "meei.\n",
      "di.\n",
      "jaceywy.\n",
      "lbyad.\n",
      "zalyl.\n",
      "rin.\n",
      "ko.\n",
      "ai.\n",
      "gzusavia.\n",
      "ynistyiathe.\n",
      "gzilelilmyniya.\n",
      "kvoviyaleorlelami.\n",
      "mlyempenddya.\n",
      "melliselay.\n",
      "ve.\n",
      "danani.\n",
      "hospayinaeirifgh.\n",
      "rlwiah.\n",
      "she.\n",
      "cdy.\n",
      "zarisaiarivinin.\n",
      "joheshan.\n",
      "bia.\n",
      "grx.\n",
      "cphr.\n",
      "mwcah.\n",
      "sor.\n",
      "zia.\n",
      "kammanaver.\n",
      "siranaz.\n",
      "jukh.\n",
      "stakay.\n",
      "ya.\n",
      "an.\n",
      "meluase.\n",
      "garurtani.\n",
      "vze.\n",
      "lelat.\n",
      "annaran.\n",
      "onan.\n",
      "k.\n",
      "sa.\n",
      "an.\n",
      "omahen.\n",
      "ddorn.\n",
      "isicha.\n",
      "eeiyala.\n",
      "dreovikan.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for i in range(100): \n",
    "    word = []   \n",
    "    idx = 0\n",
    "    while True:\n",
    "        x_feat = F.one_hot(torch.tensor(idx), num_classes=len(chars)).float()\n",
    "        O = x_feat @ W  # O shape: [vocab_size]\n",
    "        O = O.exp()\n",
    "        O_sum = O.sum()  # scalar\n",
    "        O_softmax = O / O_sum  # shape: [vocab_size]\n",
    "        O_softmax = O_softmax.unsqueeze(0)  # make it 2D for torch.multinomial\n",
    "        idx = torch.multinomial(O_softmax, 1).item()  # get integer index directly\n",
    "        word.append(itos[idx])\n",
    "        if idx == 0:\n",
    "            break\n",
    "    print(\"\".join(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "931733e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.8541e+00,  1.8222e+00,  6.0388e-01,  7.7032e-01,  8.6212e-01,\n",
      "          7.6314e-01, -5.4390e-01, -6.7240e-02,  2.0114e-01, -1.9186e-01,\n",
      "          1.2225e+00,  1.4243e+00,  7.8962e-01,  1.2693e+00,  4.7289e-01,\n",
      "         -6.0047e-01, -3.3036e-01, -1.9560e+00,  8.3143e-01,  1.0579e+00,\n",
      "          6.0541e-01, -2.0857e+00, -6.4754e-01, -8.5202e-01, -1.6498e+00,\n",
      "         -2.9200e-01,  2.6237e-01],\n",
      "        [ 2.3353e+00, -1.4721e-01, -1.7464e-01, -3.1596e-01,  4.8229e-01,\n",
      "          7.2233e-02, -1.5391e+00, -1.3371e+00,  1.2886e+00,  9.4242e-01,\n",
      "         -1.2949e+00, -1.2577e-01,  1.3694e+00,  9.3267e-01,  2.1356e+00,\n",
      "         -2.0880e+00, -1.9384e+00, -2.1924e+00,  1.6250e+00,  5.5279e-01,\n",
      "          6.4964e-02, -5.2676e-01,  2.5927e-01, -1.4926e+00, -1.2721e+00,\n",
      "          1.1596e+00, -3.9371e-01],\n",
      "        [ 4.4943e-01,  2.1200e+00, -7.8971e-01, -4.9260e-01,  3.6238e-01,\n",
      "          2.8832e+00, -6.1114e-01, -8.1363e-01,  1.8764e-01,  1.6472e+00,\n",
      "         -8.7289e-01, -4.9853e-01,  6.8334e-01, -1.1952e+00, -9.3344e-01,\n",
      "          9.3413e-01, -2.9498e-01, -2.6194e-01,  3.1408e+00, -5.4912e-01,\n",
      "         -4.5961e-01, -8.9758e-02, -1.7621e+00, -8.0303e-01, -1.0333e+00,\n",
      "          5.7133e-01, -3.2331e-01],\n",
      "        [-7.1859e-01,  2.9004e+00, -7.3471e-01,  1.5691e-01, -6.0153e-01,\n",
      "          2.4984e+00, -2.1011e+00, -7.5875e-01,  2.6907e+00,  1.7474e+00,\n",
      "         -6.1947e-01,  1.9063e+00,  2.2423e-01, -4.3090e-01, -1.4143e+00,\n",
      "          2.0971e+00, -8.0575e-01, -3.9226e-01,  4.9321e-01, -8.4264e-01,\n",
      "         -1.0094e+00, -2.7452e-01, -3.4112e-01, -1.4438e+00, -5.7314e-01,\n",
      "          7.1380e-01, -2.7203e-01],\n",
      "        [ 2.0501e+00,  2.9973e+00, -9.3675e-01, -8.6427e-01,  6.6013e-01,\n",
      "          2.9817e+00, -9.2684e-01, -1.1623e+00,  4.6399e-01,  2.3261e+00,\n",
      "         -1.2561e+00, -8.1260e-01, -7.9206e-01, -3.7884e-01, -7.5746e-01,\n",
      "          1.7229e+00, -8.8235e-01, -1.4276e+00,  1.8444e+00, -7.1813e-01,\n",
      "         -1.3106e+00,  7.4388e-02, -1.1376e+00, -1.1106e+00, -1.0087e+00,\n",
      "          1.5354e+00, -1.3899e+00],\n",
      "        [ 2.9789e+00,  1.2058e+00, -5.1472e-01, -2.9208e-01,  6.3050e-01,\n",
      "          1.8350e+00, -8.1814e-01, -5.2124e-01, -4.1479e-01,  1.3929e+00,\n",
      "         -1.1464e+00, -1.4737e-01,  2.7747e+00,  1.3309e+00,  2.5804e+00,\n",
      "          2.6880e-01, -8.2038e-01, -1.5665e+00,  2.2680e+00,  1.4443e+00,\n",
      "          1.0472e+00, -9.2878e-01,  8.2000e-01, -1.1125e+00, -4.5430e-01,\n",
      "          1.6624e+00, -1.2735e-01],\n",
      "        [ 5.8756e-01,  2.3311e+00,  3.4448e-01,  1.5566e-01,  3.6955e-01,\n",
      "          1.7450e+00, -4.9927e-01, -8.1930e-02,  2.6256e-01,  2.0229e+00,\n",
      "         -7.4932e-01, -9.9798e-01,  7.6762e-01, -1.2812e+00,  1.4899e-01,\n",
      "          5.4342e-01, -2.0612e-01, -6.5185e-01,  1.3534e+00, -2.3367e+00,\n",
      "          1.6880e-01, -1.2150e+00, -6.3489e-01, -7.0700e-02, -6.8352e-01,\n",
      "          2.2060e-01, -2.6045e-01],\n",
      "        [ 6.1434e-01,  1.8805e+00, -8.6350e-01, -5.1641e-01, -2.6319e-01,\n",
      "          1.8946e+00, -1.1837e+00, -7.3128e-01,  1.9684e+00,  1.1992e+00,\n",
      "         -1.9161e+00, -1.7239e+00, -2.4473e-01, -6.6466e-01, -6.6674e-01,\n",
      "          9.6333e-02, -1.1897e+00, -1.2918e+00,  1.2972e+00, -9.1684e-01,\n",
      "         -6.1484e-01,  2.8545e-01, -1.0043e+00, -5.8167e-01, -1.6465e+00,\n",
      "         -4.7023e-02, -4.1268e-01],\n",
      "        [ 3.8823e+00,  3.8109e+00, -1.2559e+00, -9.2774e-01, -2.5565e-01,\n",
      "          2.5935e+00, -8.1440e-01, -5.2846e-01, -5.5733e-01,  2.6736e+00,\n",
      "         -1.3977e+00, -7.6426e-01,  1.2220e+00,  6.5257e-01,  8.8324e-01,\n",
      "          1.7026e+00, -8.5062e-01, -5.8021e-01,  1.3327e+00, -2.1868e-01,\n",
      "          2.2553e-01,  1.1105e+00, -2.2948e-01, -3.8969e-01, -9.2053e-01,\n",
      "          1.2681e+00, -9.1928e-01],\n",
      "        [ 2.2447e+00,  2.2269e+00, -8.6787e-01,  6.5012e-01,  5.0266e-01,\n",
      "          1.8346e+00, -9.2309e-01,  4.7454e-01, -9.7462e-01, -1.0934e+00,\n",
      "         -1.1438e+00,  5.1405e-01,  1.6279e+00,  4.7224e-01,  2.0868e+00,\n",
      "          7.9593e-01, -1.3904e+00, -1.4541e+00,  1.1659e+00,  1.6060e+00,\n",
      "          7.1181e-01, -8.6517e-01, -3.8262e-03, -1.9296e+00, -1.0962e+00,\n",
      "          1.0793e+00,  2.7987e-02],\n",
      "        [ 5.8035e-01,  4.0509e+00, -2.3122e-01, -1.1203e+00, -8.3362e-01,\n",
      "          2.7980e+00, -6.4327e-01, -5.7453e-01, -3.2746e-02,  1.0595e+00,\n",
      "         -1.5785e+00, -2.2455e-02, -1.8502e+00, -1.3250e+00, -2.1826e+00,\n",
      "          2.8896e+00, -3.7638e-01, -3.1576e-01, -9.3823e-01, -3.9339e-02,\n",
      "         -3.8882e-01,  1.7587e+00, -1.3397e-03, -2.3332e-01, -5.2039e-01,\n",
      "         -2.6066e-01, -4.6781e-01],\n",
      "        [ 1.4707e+00,  3.0834e+00, -1.1141e+00, -1.1495e+00, -1.5044e+00,\n",
      "          2.4131e+00, -1.1507e+00, -1.4173e+00,  1.2728e+00,  1.8298e+00,\n",
      "         -1.7880e+00, -1.1421e+00,  3.7344e-01, -1.2183e+00, -8.5899e-01,\n",
      "          1.4128e+00, -1.2890e+00, -1.3359e+00,  1.3638e-01, -9.6904e-02,\n",
      "         -1.6823e+00, -9.8317e-01, -1.1868e+00, -7.8337e-01, -2.2389e+00,\n",
      "          1.5163e+00, -1.1502e+00],\n",
      "        [ 2.5402e+00,  3.2360e+00, -1.2965e+00, -9.7037e-01,  1.5506e-01,\n",
      "          3.3440e+00, -1.1788e+00, -1.2958e+00, -1.8774e+00,  3.1797e+00,\n",
      "         -1.1974e+00, -1.2041e+00,  2.5637e+00, -6.4754e-01, -1.2637e+00,\n",
      "          1.8898e+00, -1.3045e+00, -1.4515e+00, -1.2407e+00, -4.6692e-01,\n",
      "         -3.8659e-01,  1.0976e+00, -6.1938e-01, -1.1569e+00, -1.2869e+00,\n",
      "          2.7312e+00, -1.2682e+00],\n",
      "        [ 2.0993e+00,  3.7403e+00,  2.7965e-01, -6.8434e-01, -1.0722e+00,\n",
      "          2.5738e+00, -1.5439e+00, -1.0155e+00, -6.8481e-01,  3.0099e+00,\n",
      "         -7.7334e-01, -8.1483e-01, -6.6697e-01,  8.1236e-01, -1.5169e+00,\n",
      "          1.9610e+00, -4.4328e-01, -1.9900e+00,  3.1019e-01, -9.4229e-01,\n",
      "         -1.2363e+00,  6.5470e-01, -8.6359e-01, -1.1270e+00, -8.0883e-01,\n",
      "          1.4611e+00, -7.8114e-01],\n",
      "        [ 3.7786e+00,  2.9562e+00, -1.5356e+00,  2.4957e-01,  1.5025e+00,\n",
      "          2.1679e+00, -1.5061e+00,  5.2096e-01, -1.3059e+00,  2.4080e+00,\n",
      "         -1.0725e+00, -1.0184e+00,  1.1917e-01, -1.3737e+00,  2.5084e+00,\n",
      "          1.1447e+00, -1.8735e+00, -1.6699e+00, -1.0411e+00,  5.4140e-01,\n",
      "          1.0284e+00, -5.3866e-01, -8.8847e-01, -2.1483e+00, -2.0468e+00,\n",
      "          1.0784e+00, -1.6798e-01],\n",
      "        [ 1.7175e+00, -5.1332e-02, -9.6898e-02, -3.8885e-01,  1.7877e-01,\n",
      "         -1.7725e-01, -1.1090e+00, -1.1240e+00,  7.4923e-02, -1.1335e+00,\n",
      "         -1.1618e+00, -6.8334e-01,  1.3928e+00,  5.1273e-01,  2.7567e+00,\n",
      "         -2.9531e-01, -4.9666e-01, -1.4952e+00,  1.9322e+00,  1.1857e+00,\n",
      "         -2.4856e-01,  5.6908e-01,  1.1681e-01, -4.9605e-01, -9.7044e-01,\n",
      "         -4.2878e-01, -1.0075e+00],\n",
      "        [-6.6455e-01,  2.1061e+00, -2.5717e-01, -7.5532e-01, -8.2602e-01,\n",
      "          2.0957e+00,  2.2887e-01, -7.2899e-01,  1.9584e+00, -2.7289e-01,\n",
      "         -1.2218e+00, -1.6902e+00,  4.7499e-01, -5.0444e-01,  4.1689e-01,\n",
      "          6.2956e-01, -3.9732e-01,  1.2741e-01,  1.3651e+00, -1.6746e-01,\n",
      "          4.5767e-01, -1.3024e+00, -4.4593e-01, -1.3425e+00, -4.5141e-01,\n",
      "         -6.8803e-01, -1.0300e+00],\n",
      "        [ 8.8567e-01, -3.5644e-01,  1.9684e-01, -5.5647e-01, -2.6571e-01,\n",
      "         -1.1665e+00,  1.8560e-01, -1.0275e+00, -2.2770e-01,  2.6839e-01,\n",
      "         -1.7987e-01,  4.0259e-01, -1.1967e+00,  4.7710e-01,  2.3312e-01,\n",
      "          4.6197e-01, -1.6433e+00, -1.6373e+00,  4.7649e-01, -1.1581e+00,\n",
      "          1.6938e-01,  3.2379e+00, -6.6590e-01,  6.5598e-01,  5.3881e-01,\n",
      "          4.6956e-01,  2.4142e-01],\n",
      "        [ 2.2049e+00,  2.7442e+00, -1.0768e+00, -4.9648e-01,  1.3531e-01,\n",
      "          2.4149e+00, -1.7852e+00, -6.4607e-01, -3.1324e-01,  2.9975e+00,\n",
      "         -1.4821e+00, -5.5594e-01,  9.8464e-01,  1.1646e-02, -1.3703e-01,\n",
      "          1.7412e+00, -1.6012e+00, -1.3329e+00,  1.0143e+00,  1.6109e-01,\n",
      "          2.7237e-01,  4.6765e-01, -6.4931e-01, -1.3365e+00, -1.4906e+00,\n",
      "          1.6229e+00, -1.2884e+00],\n",
      "        [ 2.6410e+00,  2.6683e+00, -9.3574e-01, -3.1455e-01, -1.0003e+00,\n",
      "          2.3575e+00, -9.5591e-01, -2.0900e+00,  2.7367e+00,  2.0958e+00,\n",
      "         -9.8931e-01, -8.1671e-02,  1.1546e+00, -6.2434e-02, -9.9723e-01,\n",
      "          1.8353e+00, -6.6180e-01, -1.5244e+00, -4.3096e-01,  1.6885e+00,\n",
      "          2.2102e+00,  6.9888e-01, -9.5264e-01, -7.7916e-01, -1.2159e+00,\n",
      "          8.7507e-01, -1.1956e+00],\n",
      "        [ 1.9602e+00,  2.7356e+00, -1.2129e+00, -9.1549e-01, -8.6698e-01,\n",
      "          2.3673e+00, -1.0827e+00, -1.0452e+00,  2.2631e+00,  2.0608e+00,\n",
      "         -8.5601e-01, -1.0806e+00,  5.3105e-01, -1.0781e+00, -5.9467e-01,\n",
      "          2.2945e+00, -1.6010e+00, -8.0345e-01,  1.6263e+00, -9.4627e-01,\n",
      "          1.6908e+00, -2.6329e-01, -8.4692e-01, -1.6115e+00, -2.4699e+00,\n",
      "          1.5918e+00,  2.5172e-01],\n",
      "        [ 5.7125e-01,  8.9218e-01, -3.0900e-01,  3.4995e-01,  6.0096e-01,\n",
      "          9.2906e-01, -3.4118e-01, -2.2854e-01, -1.0933e-01,  5.8024e-01,\n",
      "         -3.8904e-01,  4.2701e-01,  1.4867e+00,  5.7932e-01,  1.4021e+00,\n",
      "         -6.9158e-01, -6.4032e-01, -5.3500e-01,  1.8142e+00,  1.9512e+00,\n",
      "         -2.7328e-01, -8.7946e-01, -2.2906e-01, -7.8634e-01, -1.5853e-01,\n",
      "         -6.7291e-01, -1.0005e-01],\n",
      "        [ 1.0457e+00,  3.2348e+00, -3.8239e-01, -5.6222e-02, -2.8061e-01,\n",
      "          3.1065e+00, -8.8019e-01, -2.2531e-01, -7.2958e-01,  3.5971e+00,\n",
      "         -1.3660e+00, -5.5128e-01, -1.1435e+00, -1.1365e+00, -6.8060e-01,\n",
      "          1.3555e+00, -1.6509e+00, -1.7764e+00,  4.7804e-01, -1.5971e+00,\n",
      "         -2.1273e+00, -4.6725e-01, -4.8672e-01, -9.1430e-01, -9.2177e-01,\n",
      "          1.2966e+00, -3.0437e-01],\n",
      "        [ 9.1366e-01,  2.5760e+00,  2.1199e-01, -8.6876e-01, -1.7705e+00,\n",
      "          1.7604e+00,  3.1384e-01,  5.8809e-01, -8.8905e-01,  1.7878e+00,\n",
      "         -2.2836e-01, -1.9105e-01,  5.7676e-01, -1.2185e+00, -8.3391e-02,\n",
      "         -1.8685e-01, -1.3211e+00, -1.7941e-01,  7.0516e-01,  6.5872e-01,\n",
      "         -1.4745e+00, -3.2643e-01, -4.6488e-01, -1.5542e+00, -1.8154e-01,\n",
      "          1.2165e+00, -5.0953e-02],\n",
      "        [ 2.3293e+00,  1.2420e+00, -1.0410e-01, -1.7705e-01,  8.0389e-01,\n",
      "          4.9186e-01,  2.6065e-01, -5.4484e-01,  5.4661e-01,  1.7761e+00,\n",
      "         -1.2909e+00,  3.4315e-01,  7.0932e-01, -8.5600e-01,  5.1447e-01,\n",
      "          1.4524e+00,  7.3607e-03, -2.2333e-01, -1.1310e+00,  1.7802e-01,\n",
      "          1.5167e+00, -7.1210e-01, -1.1975e+00,  2.2173e-01,  1.4188e+00,\n",
      "          6.0098e-01, -1.4375e-02],\n",
      "        [ 3.0691e+00,  3.1349e+00, -6.7581e-01,  4.4234e-02,  1.0367e+00,\n",
      "          1.1410e+00, -1.6158e+00, -7.6317e-01, -8.3804e-01,  6.6644e-01,\n",
      "         -9.3605e-01, -1.7816e-01,  2.4681e+00,  4.1924e-01,  2.9742e+00,\n",
      "          1.0310e+00, -8.6535e-01, -9.4398e-01,  1.1045e+00,  1.4396e+00,\n",
      "          5.5486e-02,  2.4899e-01,  1.1624e-03, -1.4439e+00, -7.6790e-01,\n",
      "         -6.8919e-01, -4.1480e-01],\n",
      "        [ 1.0842e+00,  3.0636e+00, -7.2447e-01, -8.6630e-01, -1.3581e+00,\n",
      "          2.1845e+00, -5.2877e-01, -4.8379e-01,  1.6321e-01,  2.1662e+00,\n",
      "         -7.9205e-01, -4.3746e-01,  8.3360e-01,  7.0764e-02, -6.6100e-01,\n",
      "          2.0061e-01, -4.2111e-01, -1.7281e+00, -2.9295e-01, -5.8133e-01,\n",
      "         -3.6625e-01,  2.9677e-01, -5.9242e-01, -6.0385e-01, -6.1938e-01,\n",
      "          8.0341e-01,  1.8401e-01]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62233f56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aabd8a4d",
   "metadata": {},
   "source": [
    "chapter 2 - MLP ----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549f5742",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
